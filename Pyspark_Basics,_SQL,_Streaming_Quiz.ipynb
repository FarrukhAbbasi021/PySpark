{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz 1"
      ],
      "metadata": {
        "id": "Kk8-hNJwaVD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwT-9QTBGgLP",
        "outputId": "cf9e0cbe-9b42-4b6a-b11f-d32d1d4c819a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "tnLz0BLKUztb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Count the number of words starting with a specific letter."
      ],
      "metadata": {
        "id": "rddZ7I90bIVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"word_count\").getOrCreate()\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "text_df = spark.read.csv(\"SPAM SMS.csv\")\n",
        "print(text_df)\n",
        "# Split the lines into words\n",
        "words_df = text_df.selectExpr(\"explode(split(_c0, ' ')) as word\")\n",
        "print(words_df)\n",
        "# Count the number of words starting with a specific letter (e.g., 'n')\n",
        "letter_count = words_df.filter(col(\"word\").startswith(\"h\")).count()\n",
        "\n",
        "print(f\"Number of words starting with 'h': {letter_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVL8ydjRGvLW",
        "outputId": "1ea7baf1-00eb-4fb8-c290-683495af2f97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]\n",
            "DataFrame[word: string]\n",
            "Number of words starting with 'h': 4827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Count the number of occurrences of a specific word."
      ],
      "metadata": {
        "id": "YJo5-eT0bRXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the word to count\n",
        "target_word =\"ham\"\n",
        "\n",
        "# Count the occurrences of the specific word\n",
        "word_count = words_df.filter(col(\"word\") == target_word).count()\n",
        "\n",
        "print(f\"Occurrences of '{target_word}': {word_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9On_ucMGXo92",
        "outputId": "2c7fc442-1d7b-4c1f-8542-c8165d93c7bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Occurrences of 'ham': 4825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Convert all words to uppercase and count the occurrences."
      ],
      "metadata": {
        "id": "Mdz856WPIt9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert all words to uppercase\n",
        "uppercase_words_df = words_df.selectExpr(\"upper(word) as word\")\n",
        "\n",
        "# Count the occurrences of each word\n",
        "uppercase_word_counts = uppercase_words_df.groupBy(\"word\").count()\n",
        "\n",
        "uppercase_word_counts.show()"
      ],
      "metadata": {
        "id": "lXHS0hEtbnnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a511aa19-1220-4b49-ff99-e6902b077bf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  word|count|\n",
            "+------+-----+\n",
            "|  SPAM|  747|\n",
            "|HAM\"\"\"|    2|\n",
            "|   HAM| 4825|\n",
            "|    V1|    1|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Filter out words of length less than 5 and count the occurrences."
      ],
      "metadata": {
        "id": "VzY6ff7UQIho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length\n",
        "\n",
        "# Filter out words of length less than 5\n",
        "filtered_words_df = words_df.filter(length(\"word\") >= 5)\n",
        "\n",
        "# Count the occurrences of each filtered word\n",
        "filtered_word_counts = filtered_words_df.groupBy(\"word\").count()\n",
        "\n",
        "filtered_word_counts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ri0HxptQCjz",
        "outputId": "3ec245fb-03a1-4293-9152-b3b4e6423f24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  word|count|\n",
            "+------+-----+\n",
            "|ham\"\"\"|    2|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Count the number of lines containing a specific word."
      ],
      "metadata": {
        "id": "US4OguhJRVFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the word to check in lines\n",
        "word_in_lines = \"spam\"\n",
        "\n",
        "# Specify the correct column name containing the text\n",
        "text_column_name = \"_c0\"\n",
        "\n",
        "# Count the number of lines containing the specific word\n",
        "lines_with_word_count = text_df.filter(col(text_column_name).contains(word_in_lines)).count()\n",
        "\n",
        "print(f\"Number of lines containing '{word_in_lines}': {lines_with_word_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DndInIEeRhZE",
        "outputId": "fa19164a-15d9-4492-9c80-9b57a90df82b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines containing 'spam': 747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "UbeXSpgyWiCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.streaming import StreamingQuery\n",
        "\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.appName('Person Data').getOrCreate()\n",
        "\n",
        "# Create a sample dataset\n",
        "data = [(\"John\", \"Doe\", 22),\n",
        "        (\"Jane\", \"Doe\", 28),\n",
        "        (\"Bob\", \"Johnson\", 30),\n",
        "        (\"Alice\", \"Smith\", 21),\n",
        "        (\"James\", \"Johnson\", 26)]\n",
        "\n",
        "columns = ['FirstName','LastName','Age']\n",
        "\n",
        "df= spark.createDataFrame(data,columns)\n",
        "\n",
        "# Task 1: Select people younger than 25 years old\n",
        "young_people =df.filter(col(\"Age\")< 25)\n",
        "\n",
        "# Task 2: Find the average age of people whose last name starts with the letter 'D'\n",
        "average_age_D = df.filter(col(\"LastName\").startswith(\"D\")).agg(F.avg(\"Age\").alias(\"AverageAge\"))\n",
        "\n",
        "# Task 3: Get the dataset's total count of distinct last names\n",
        "distinct_last_names_count = df.select(\"LastName\").distinct().count()\n",
        "\n",
        "# Task 4: Find the maximum age of people whose first name starts with the letter 'J'\n",
        "max_age_J = df.filter(col(\"FirstName\").startswith(\"J\")).agg(F.max(\"Age\").alias(\"MaxAge\"))\n",
        "\n",
        "# Display results\n",
        "print(\"Young People:\")\n",
        "young_people.show()\n",
        "\n",
        "print(\"Average Age of People with Last Name starting with 'D':\")\n",
        "average_age_D.show()\n",
        "\n",
        "print(\"Total Count of Distinct Last Names:\")\n",
        "print(distinct_last_names_count)\n",
        "\n",
        "print(\"Maximum Age of People with First Name starting with 'J':\")\n",
        "max_age_J.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Va46-nuS_2E",
        "outputId": "8834e747-ca99-4bf2-e3fd-590b24211ffc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Young People:\n",
            "+---------+--------+---+\n",
            "|FirstName|LastName|Age|\n",
            "+---------+--------+---+\n",
            "|     John|     Doe| 22|\n",
            "|    Alice|   Smith| 21|\n",
            "+---------+--------+---+\n",
            "\n",
            "Average Age of People with Last Name starting with 'D':\n",
            "+----------+\n",
            "|AverageAge|\n",
            "+----------+\n",
            "|      25.0|\n",
            "+----------+\n",
            "\n",
            "Total Count of Distinct Last Names:\n",
            "3\n",
            "Maximum Age of People with First Name starting with 'J':\n",
            "+------+\n",
            "|MaxAge|\n",
            "+------+\n",
            "|    28|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "hkavudJzadFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,when\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame with sample data\n",
        "data = [(\"John\", \"Smith\", 25),\n",
        "        (\"Jane\", \"Johnson\", 30),\n",
        "        (\"Alice\", \"Jones\", 22),\n",
        "        (\"Jack\", \"Jackson\", 28),\n",
        "        (\"Jerry\", \"Johnson\", 25),\n",
        "        # Add more data as needed\n",
        "        ]\n",
        "\n",
        "columns = [\"first_name\", \"last_name\", \"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Task 1: Get the sum of ages of people whose last name contains 's'\n",
        "sum_ages_with_s = df.filter(col(\"last_name\").like(\"%s%\")).agg({\"age\": \"sum\"}).collect()[0][0]\n",
        "print(f\"Sum of ages of people whose last name contains 's': {sum_ages_with_s}\")\n",
        "\n",
        "# Task 2: Get the number of distinct first names that start with 'J' and end with 'e'\n",
        "distinct_j_first_names = df.filter((col(\"first_name\").startswith(\"J\")) & (col(\"first_name\").endswith(\"e\"))).select(\"first_name\").distinct().count()\n",
        "print(f\"Number of distinct first names starting with 'J' and ending with 'e': {distinct_j_first_names}\")\n",
        "\n",
        "# Task 3: Get the number of distinct last names that start with 'J' and end with 'n'\n",
        "distinct_j_last_names = df.filter((col(\"last_name\").startswith(\"J\")) & (col(\"last_name\").endswith(\"n\"))).select(\"last_name\").distinct().count()\n",
        "print(f\"Number of distinct last names starting with 'J' and ending with 'n': {distinct_j_last_names}\")\n",
        "\n",
        "# Task 4: Get the first and last names of the person aged 25\n",
        "names_of_age_25 = df.filter(col(\"age\") == 25).select(\"first_name\", \"last_name\").collect()\n",
        "print(f\"First and last names of people aged 25: {names_of_age_25}\")\n",
        "\n",
        "# You can also use SQL queries with the DataFrame\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# SQL query example:\n",
        "result = spark.sql(\"SELECT * FROM people WHERE age = 25\")\n",
        "result.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpgrFFm0ZL7j",
        "outputId": "ee7abf2e-3d15-4ab2-a2ce-711dd4c57858"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---+\n",
            "|first_name|last_name|age|\n",
            "+----------+---------+---+\n",
            "|      John|    Smith| 25|\n",
            "|      Jane|  Johnson| 30|\n",
            "|     Alice|    Jones| 22|\n",
            "|      Jack|  Jackson| 28|\n",
            "|     Jerry|  Johnson| 25|\n",
            "+----------+---------+---+\n",
            "\n",
            "Sum of ages of people whose last name contains 's': 105\n",
            "Number of distinct first names starting with 'J' and ending with 'e': 1\n",
            "Number of distinct last names starting with 'J' and ending with 'n': 2\n",
            "First and last names of people aged 25: [Row(first_name='John', last_name='Smith'), Row(first_name='Jerry', last_name='Johnson')]\n",
            "+----------+---------+---+\n",
            "|first_name|last_name|age|\n",
            "+----------+---------+---+\n",
            "|      John|    Smith| 25|\n",
            "|     Jerry|  Johnson| 25|\n",
            "+----------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part 4"
      ],
      "metadata": {
        "id": "p0u7H5kabjxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"first_name\", StringType(), True),\n",
        "    StructField(\"last_name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"Alex\", \"Smith\", 40),\n",
        "    (\"Jane\", \"Doe\", 30),\n",
        "    (\"John\", \"Doe\", 35),\n",
        "    (\"Mike\", \"Johnston\", 25),\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# Update the age of 'Alex Smith' to 41\n",
        "df = df.withColumn(\"age\", when((col(\"first_name\") == \"Alex\") & (col(\"last_name\") == \"Smith\"), 41).otherwise(col(\"age\")))\n",
        "\n",
        "# Update the first name of 'Jane Doe' to 'Janet'\n",
        "df = df.withColumn(\"first_name\", when((col(\"first_name\") == \"Jane\") & (col(\"last_name\") == \"Doe\"), \"Janet\").otherwise(col(\"first_name\")))\n",
        "\n",
        "# Delete 'John Doe' from the table\n",
        "df = df.filter((col(\"first_name\") != \"John\") & (col(\"last_name\") != \"Doe\"))\n",
        "\n",
        "# Delete 'Mike Johnston' from the table\n",
        "df = df.filter((col(\"first_name\") != \"Mike\") & (col(\"last_name\") != \"Johnston\"))\n",
        "\n",
        "# Show the updated DataFrame\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y--WWGimbBPO",
        "outputId": "b9b7d98a-f612-4b3c-dd34-9ec92aa336c7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+----------+---------+---+\n",
            "|first_name|last_name|age|\n",
            "+----------+---------+---+\n",
            "|      Alex|    Smith| 40|\n",
            "|      Jane|      Doe| 30|\n",
            "|      John|      Doe| 35|\n",
            "|      Mike| Johnston| 25|\n",
            "+----------+---------+---+\n",
            "\n",
            "\n",
            "Updated DataFrame:\n",
            "+----------+---------+---+\n",
            "|first_name|last_name|age|\n",
            "+----------+---------+---+\n",
            "|      Alex|    Smith| 41|\n",
            "+----------+---------+---+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}