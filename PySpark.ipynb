{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2MEL_cOMx5wm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36548bf6-3be0-4522-fbf1-78549ec127ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "metadata": {
        "id": "yUPkeVs8yZDN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Session\n",
        "spark= SparkSession.builder \\\n",
        "      .appName('PySparkTutorial')\\\n",
        "      .getOrCreate()\n",
        "\n",
        "# checking type\n",
        "type(spark)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUC8xreWyuAX",
        "outputId": "db79fbcc-646c-4de0-e558-76aff9d28d68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.session.SparkSession"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating rdd\n",
        "data=[1,2,3,4,5]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "type(rdd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP7IG69by9NO",
        "outputId": "99c03cac-8b4c-4c03-dec8-e6241b7e85ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tranforming rdd and adding 1 to each\n",
        "rdd_transformed = rdd.map(lambda x:x+1)\n",
        "\n",
        "collected_data=rdd_transformed.collect()\n",
        "\n",
        "# Displaying Original & Transformed data\n",
        "orignal_data=rdd.collect()\n",
        "print(orignal_data,collected_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnxg94tr0sy2",
        "outputId": "40c0ce42-528c-4752-891e-762910180c31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5] [2, 3, 4, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter\n",
        "rdd_filtered = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Count\n",
        "count_orignal = rdd.count()\n",
        "count_filtered = rdd_filtered.count()\n",
        "\n",
        "# First Element\n",
        "first_element= rdd.first()\n",
        "\n",
        "#Result\n",
        "print(count_orignal,count_filtered,first_element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7w_Crhq1G2K",
        "outputId": "325c654d-9257-4883-9bae-739095e75bd7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 2 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating  an RDd\n",
        "data=[1,2,3,4,5]\n",
        "rdd =spark.sparkContext.parallelize(data)\n",
        "\n",
        "#sqare Tranormation\n",
        "rdd_squared = rdd.map(lambda x:x**2)\n",
        "\n",
        "#Collecting & Displaying Orignal and transformed rdd\n",
        "orignal_data = rdd.collect()\n",
        "squared_data = rdd_squared.collect()\n",
        "print(orignal_data,squared_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSMmP0TO2ouV",
        "outputId": "bceb62bb-eb09-4c3f-a24a-88ca97b735a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5] [1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Filter Transformation\n",
        "rdd_even = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "#Diaplayig Filtered Rdd\n",
        "even_data = rdd_even.collect()\n",
        "print(orignal_data,even_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py1j4YdN930O",
        "outputId": "463e13f3-ed57-427f-efd1-c5732be3ddbf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5] [2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "sentences = ['hello world', 'I Love Pyspark', \"i love computers\",'I love Coding']\n",
        "rdd_sentence = spark.sparkContext.parallelize(sentences)\n",
        "\n",
        "# Flatmap transformaation\n",
        "rdd_words = rdd_sentence.flatMap(lambda  x: x.split(' '))\n",
        "\n",
        "#Results\n",
        "origanl_senetence = rdd_sentence.collect()\n",
        "words = rdd_words.collect()\n",
        "print(origanl_senetence,words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFdAoSUW-vdR",
        "outputId": "304c4da1-80ad-4b5a-97b7-6f8f1d38a029"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello world', 'I Love Pyspark', 'i love computers', 'I love Coding'] ['hello', 'world', 'I', 'Love', 'Pyspark', 'i', 'love', 'computers', 'I', 'love', 'Coding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Two Rdds\n",
        "rdd1 =spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "rdd2 =spark.sparkContext.parallelize([4,5,6,7,8])\n",
        "\n",
        "# Applying union\n",
        "rdd_union = rdd1.union(rdd2)\n",
        "\n",
        "#collecting Results\n",
        "data_rdd1 = rdd1.collect()\n",
        "data_rdd2 = rdd2.collect()\n",
        "union_data = rdd_union.collect()\n",
        "print(data_rdd1,data_rdd2,union_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLkUlOILATD0",
        "outputId": "9c660bdf-c37a-4a08-c72c-4ec92f82e60c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5] [4, 5, 6, 7, 8] [1, 2, 3, 4, 5, 4, 5, 6, 7, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD with duplicates\n",
        "rdd_with_duplicates = spark.sparkContext.parallelize([1,2,3,1,2,3,4,5,6,6,5,4])\n",
        "\n",
        "# Appling distinct Transformation\n",
        "rdd_distinct = rdd_with_duplicates.distinct()\n",
        "\n",
        "#Results\n",
        "Data_with_duplicates = rdd_with_duplicates.collect()\n",
        "Data_with_distinct = rdd_distinct.collect()\n",
        "print(Data_with_duplicates,Data_with_distinct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m07TQ1-YCaol",
        "outputId": "68645270-6668-4d7c-d62b-e4f8cd30fdd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 1, 2, 3, 4, 5, 6, 6, 5, 4] [2, 4, 6, 1, 3, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "rdd= spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "# Appling reduce transformation\n",
        "sum_result = rdd.reduce(lambda x,y: x+y)\n",
        "\n",
        "# Results\n",
        "orignal_data = rdd.collect()\n",
        "print(orignal_data,sum_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUm0RxWfDvXg",
        "outputId": "23469c40-6f73-49c8-c208-8a0190eb8e14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5] 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an Rdd\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "#Transformation\n",
        "rdd_transformed = rdd.filter(lambda x:x % 2 == 0).map(lambda x:x**2 == 0)\n",
        "\n",
        "# Results\n",
        "collected_data = rdd_transformed.collect()\n",
        "print(collected_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOdCuCq8GCL2",
        "outputId": "c65d1855-fea0-44bf-fe57-f187335f64bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an Rdd\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "# Transformation\n",
        "rdd_filtered = rdd.filter(lambda x:x % 2 == 0)\n",
        "\n",
        "# Appling count and filtered\n",
        "orignal_count = rdd.count()\n",
        "orignal_filtered = rdd_filtered.count()\n",
        "\n",
        "# Results\n",
        "print(orignal_count,orignal_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlouwbB4HgQd",
        "outputId": "1b607ce1-584e-48a4-a24d-1e86c9a64327"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "rdd = spark.sparkContext.parallelize([10,20,30,40,50])\n",
        "\n",
        "#Applying First Action\n",
        "first_element = rdd.first()\n",
        "\n",
        "#Results\n",
        "first_element"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoyO94m5Ih54",
        "outputId": "ebc93191-d5c5-4e96-ce1c-a799c14622d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "rdd = spark.sparkContext.parallelize([10,20,30,40,50,60,70,80,90,100])\n",
        "\n",
        "#Retriving first 3 Elements\n",
        "first_n_element = rdd.take(3)\n",
        "\n",
        "#Results\n",
        "first_n_element"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL83nHBpJN40",
        "outputId": "c12d9167-ce61-483e-bacf-59b7804183df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 20, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "\n",
        "# Applying square\n",
        "rdd_squared = rdd.map(lambda x:x**2 == 0)\n",
        "\n",
        "#Results\n",
        "orignal_data = rdd.collect()\n",
        "squared_data = rdd_squared.collect()\n",
        "print(orignal_data,squared_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvzAnvQ1Jsl-",
        "outputId": "b78dc0dd-9308-422e-aa6f-f334dcdaa637"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5] [False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "# Applying filtere to get even numbers\n",
        "rdd_even =rdd.filter(lambda x:x % 2 == 0)\n",
        "\n",
        "#Results\n",
        "orignal_data = rdd.collect()\n",
        "even_data = rdd_even.collect()\n",
        "print(orignal_data,even_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUsupszjKjBe",
        "outputId": "4d427698-627a-4127-911b-ca7b941a997a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "rdd = spark.sparkContext.parallelize([1,2,3])\n",
        "\n",
        "# Applying Flatmap\n",
        "rdd_flatmapped = rdd.flatMap(lambda x: (x,x*100))\n",
        "\n",
        "# Results\n",
        "orignal_data = rdd.collect()\n",
        "flatmapped_data = rdd_flatmapped.collect()\n",
        "print(orignal_data,flatmapped_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2BmQEE6LQff",
        "outputId": "52e1d248-16a3-4a8c-c298-cd395545dcbf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3] [1, 100, 2, 200, 3, 300]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "rdd = spark.sparkContext.parallelize([('a',1),('b',2),('c',3),('d',4),('e',5)])\n",
        "\n",
        "#Applying groupby key transformation\n",
        "rdd_grouped = rdd.groupByKey()\n",
        "\n",
        "# Results\n",
        "orignal_data = rdd.collect()\n",
        "grouped_data = [(k, list(v)) for k ,v in rdd_grouped.collect()]\n",
        "print(orignal_data,grouped_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK8NC0OqL_1G",
        "outputId": "0884114a-c870-428e-e362-8e31ae480798"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5)] [('b', [2]), ('c', [3]), ('d', [4]), ('a', [1]), ('e', [5])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD\n",
        "rdd = spark.sparkContext.parallelize([('a',1),('b',2),('c',3),('d',4),('e',5)])\n",
        "\n",
        "# Applying reduceby key\n",
        "rdd_reduced = rdd.reduceByKey(lambda x,y : x+y)\n",
        "\n",
        "# Results\n",
        "orignal_data = rdd.collect()\n",
        "reduced_data = rdd_reduced.collect()\n",
        "print(orignal_data,reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CznQ8aElNRwl",
        "outputId": "aa9e5f08-d65a-498b-cbae-7dc3ec103a15"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5)] [('b', 2), ('c', 3), ('d', 4), ('a', 1), ('e', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restarting"
      ],
      "metadata": {
        "id": "zFGZGGGEjG6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName('IntegerTypeExamples').getOrCreate()\n",
        "# Creating a DataFrame with integer type\n",
        "data = [(10,), (15,), (20,), (25,), (30,)]\n",
        "schema = ['Age']\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "oqJGR3KROHCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fd2de3-501b-4c69-a272-e78c7c565402"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|Age|\n",
            "+---+\n",
            "| 10|\n",
            "| 15|\n",
            "| 20|\n",
            "| 25|\n",
            "| 30|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 1\n",
        "\n",
        "# Data of Family Memebers\n",
        "family_ages = [(35,),(40,),(20,),(12,)]\n",
        "\n",
        "# Creating DF\n",
        "family_df = spark.createDataFrame(family_ages, schema=['age'])\n",
        "family_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShctfEXVlPlA",
        "outputId": "14b10d51-fbdc-47e1-cb6a-acabfd987c20"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|age|\n",
            "+---+\n",
            "| 35|\n",
            "| 40|\n",
            "| 20|\n",
            "| 12|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "# Data of family members' names and ages\n",
        "family_data = [('Alice', 35), ('Bob', 40), ('Charlie', 10), ('Diana', 12)]\n",
        "# Defining the schema\n",
        "family_schema = StructType([\n",
        "StructField('Name', StringType(), True),\n",
        "StructField('Age', IntegerType(), True)\n",
        "])\n",
        "# Creating a DataFrame\n",
        "family_df = spark.createDataFrame(family_data, schema=family_schema)\n",
        "# Displaying the DataFrame\n",
        "family_df.show()"
      ],
      "metadata": {
        "id": "CyLuKR7inof2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114a9117-a9f2-462f-f3dc-2b4788a1bad9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 35|\n",
            "|    Bob| 40|\n",
            "|Charlie| 10|\n",
            "|  Diana| 12|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "filtered_family_df = family_df.filter(family_df['Age']>20)\n",
        "filtered_family_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_SK_KjQp_Hg",
        "outputId": "6a109e53-9168-4307-a956-f4ef5e3fbb46"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 35|\n",
            "|  Bob| 40|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "# Data of plants and the amount of water they need\n",
        "plants_data = [('Rose', 1.5), ('Tulip', 2.75), ('Daisy', 1.25), ('Sunflower',4.45)]\n",
        "# Defining the schema\n",
        "plants_schema = StructType([\n",
        "StructField('Plant', StringType(), True),\n",
        "StructField('Water_Needed', FloatType(), True)\n",
        "])\n",
        "\n",
        "#Creating DataFrame\n",
        "plants_df = spark.createDataFrame(plants_data, schema=plants_schema)\n",
        "plants_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne8T9AdBwPu-",
        "outputId": "f5856599-b37c-401c-8965-1d47c24de505"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|    Plant|Water_Needed|\n",
            "+---------+------------+\n",
            "|     Rose|         1.5|\n",
            "|    Tulip|        2.75|\n",
            "|    Daisy|        1.25|\n",
            "|Sunflower|        4.45|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 1: More Plants\n",
        "\n",
        "#Adding more plants\n",
        "more_plants_data = [('Liliy',2.5),('Orchid',3.5),('Voilet',5.4),('Jasmine',8.9)]\n",
        "\n",
        "# Adding to Dataframe\n",
        "more_plants_df = spark.createDataFrame(more_plants_data,schema=plants_schema)\n",
        "all_plants_df = plants_df.union(more_plants_df)\n",
        "\n",
        "#Diaplaying updated Dataframe\n",
        "all_plants_df.show()\n"
      ],
      "metadata": {
        "id": "oLJxHqo2x461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbdbe25-f299-481d-f282-28c46f25fcde"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|    Plant|Water_Needed|\n",
            "+---------+------------+\n",
            "|     Rose|         1.5|\n",
            "|    Tulip|        2.75|\n",
            "|    Daisy|        1.25|\n",
            "|Sunflower|        4.45|\n",
            "|    Liliy|         2.5|\n",
            "|   Orchid|         3.5|\n",
            "|   Voilet|         5.4|\n",
            "|  Jasmine|         8.9|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 2\n",
        "thirsty_plants_df = all_plants_df.filter(all_plants_df['Water_Needed']>2)\n",
        "thirsty_plants_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI-5a_-eSnIt",
        "outputId": "5c07087a-b44e-4eb4-b133-f408f32a9895"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|    Plant|Water_Needed|\n",
            "+---------+------------+\n",
            "|    Tulip|        2.75|\n",
            "|Sunflower|        4.45|\n",
            "|    Liliy|         2.5|\n",
            "|   Orchid|         3.5|\n",
            "|   Voilet|         5.4|\n",
            "|  Jasmine|         8.9|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 3\n",
        "sorted_plants_df = all_plants_df.sort(all_plants_df['Water_Needed'].desc())\n",
        "sorted_plants_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqgNOd8YVJ70",
        "outputId": "5c25f930-5914-4cea-aee8-082081bafd5f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|    Plant|Water_Needed|\n",
            "+---------+------------+\n",
            "|  Jasmine|         8.9|\n",
            "|   Voilet|         5.4|\n",
            "|Sunflower|        4.45|\n",
            "|   Orchid|         3.5|\n",
            "|    Tulip|        2.75|\n",
            "|    Liliy|         2.5|\n",
            "|     Rose|         1.5|\n",
            "|    Daisy|        1.25|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import Struct\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Data of guest& gifts\n",
        "party_data = [('Alice',3),('Bob',5),('Charlie',2),('Diana',1)]\n",
        "\n",
        "#Defining Schema\n",
        "party_schema=StructType([\n",
        "    StructField('Guest',StringType(),True),\n",
        "    StructField('Gifts',IntegerType(),True)\n",
        "])\n",
        "\n",
        "party_df=  spark.createDataFrame(party_data,party_schema)\n",
        "party_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOzzW1gPVt5t",
        "outputId": "576cd4bb-4649-4152-ae29-88cdb42ed9e2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  Guest|Gifts|\n",
            "+-------+-----+\n",
            "|  Alice|    3|\n",
            "|    Bob|    5|\n",
            "|Charlie|    2|\n",
            "|  Diana|    1|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 1\n",
        "more_guests_data = [('Eva',4),('Frank',3),('Grace',2),('Henry',5)]\n",
        "\n",
        "# Ading to Dataframe\n",
        "more_guests_df = spark.createDataFrame(more_guests_data,schema= party_schema)\n",
        "all_guests_df = party_df.union(more_guests_df)\n",
        "all_guests_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LoV5HmtXi_J",
        "outputId": "2f3976ca-4831-48a5-fa54-762f54b4ae64"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  Guest|Gifts|\n",
            "+-------+-----+\n",
            "|  Alice|    3|\n",
            "|    Bob|    5|\n",
            "|Charlie|    2|\n",
            "|  Diana|    1|\n",
            "|    Eva|    4|\n",
            "|  Frank|    3|\n",
            "|  Grace|    2|\n",
            "|  Henry|    5|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generous_guest_df = all_guests_df.filter(all_guests_df['Gifts']>2)\n",
        "generous_guest_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op2tTIYNYw2w",
        "outputId": "fd013581-e042-4fab-e8d3-cc38e58c4b88"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|Guest|Gifts|\n",
            "+-----+-----+\n",
            "|Alice|    3|\n",
            "|  Bob|    5|\n",
            "|  Eva|    4|\n",
            "|Frank|    3|\n",
            "|Henry|    5|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_guests_df=all_guests_df.sort(all_guests_df['Gifts'])\n",
        "sorted_guests_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFpMukLbZLeJ",
        "outputId": "68c625f3-30e3-4ba1-d621-8231dccfc349"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  Guest|Gifts|\n",
            "+-------+-----+\n",
            "|  Diana|    1|\n",
            "|  Grace|    2|\n",
            "|Charlie|    2|\n",
            "|  Frank|    3|\n",
            "|  Alice|    3|\n",
            "|    Eva|    4|\n",
            "|  Henry|    5|\n",
            "|    Bob|    5|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Books Data\n",
        "books_data =[('To Kill a MockingBird','Harper lee'),('1984','George owell')]\n",
        "\n",
        "# Defining Schema\n",
        "books_schema = StructType([\n",
        "    StructField('Title',StringType(),True),\n",
        "    StructField('Author',StringType(),True)\n",
        "])\n",
        "\n",
        "\n",
        "# creating dataframe\n",
        "books_df = spark.createDataFrame(books_data,schema=books_schema)\n",
        "books_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEg-nL85ZkNy",
        "outputId": "5134f8d0-e302-4d7d-f7d3-880ecbecd46f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+\n",
            "|               Title|      Author|\n",
            "+--------------------+------------+\n",
            "|To Kill a Mocking...|  Harper lee|\n",
            "|                1984|George owell|\n",
            "+--------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 1\n",
        "\n",
        "# Adding more Books\n",
        "more_books_data =[('the books','john'),('The Ride', 'joseph')]\n",
        "\n",
        "# Addding to dataframe\n",
        "more_books_df = spark.createDataFrame(more_books_data,schema=books_schema)\n",
        "all_books_df = books_df.union(more_books_df)\n",
        "\n",
        "all_books_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZoj-EXqcLur",
        "outputId": "6c0e743f-93b1-4a43-c635-382548ef96ae"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+\n",
            "|               Title|      Author|\n",
            "+--------------------+------------+\n",
            "|To Kill a Mocking...|  Harper lee|\n",
            "|                1984|George owell|\n",
            "|           the books|        john|\n",
            "|            The Ride|      joseph|\n",
            "+--------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 2\n",
        "orwell_books_df =all_books_df.filter(all_books_df['Author'] == 'john')\n",
        "orwell_books_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUs5GrgeOhW",
        "outputId": "3cf1399d-8166-4696-a622-e52da38e4c8f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|    Title|Author|\n",
            "+---------+------+\n",
            "|the books|  john|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 3\n",
        "sorted_books_df = all_books_df.sort(all_books_df['Title'])\n",
        "sorted_books_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSht7GOSe-cB",
        "outputId": "7e715052-3857-4609-b769-49f3c4b715a7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+\n",
            "|               Title|      Author|\n",
            "+--------------------+------------+\n",
            "|                1984|George owell|\n",
            "|            The Ride|      joseph|\n",
            "|To Kill a Mocking...|  Harper lee|\n",
            "|           the books|        john|\n",
            "+--------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import BooleanType\n",
        "# Data of students and whether they have submitted the assignment\n",
        "student_data = [('Alice', True), ('Bob', False), ('Catherine', True), ('David',True)]\n",
        "# Defining the schema\n",
        "student_schema = StructType([\n",
        "StructField('Name', StringType(), True),\n",
        "StructField('Assignment_Submitted', BooleanType(), True)\n",
        "])\n",
        "# Creating a DataFrame\n",
        "student_df = spark.createDataFrame(student_data, schema=student_schema)\n",
        "# Displaying the DataFrame\n",
        "student_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AZkYkNLfsFk",
        "outputId": "e185e283-ea45-4b85-b5a5-9d17bf76f3ac"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|     Name|Assignment_Submitted|\n",
            "+---------+--------------------+\n",
            "|    Alice|                true|\n",
            "|      Bob|               false|\n",
            "|Catherine|                true|\n",
            "|    David|                true|\n",
            "+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 1\n",
        "more_students_data = [('Eva',True),('Frank',False),('Ali',True),('Usman',True)]\n",
        "\n",
        "# Adding to DataFrame\n",
        "more_students_df =spark.createDataFrame(more_students_data,schema=student_schema)\n",
        "all_students_df =student_df.union(more_students_df)\n",
        "\n",
        "all_students_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKkIlfgh0h7",
        "outputId": "55dbbdc4-090a-4ee3-f29a-5e422fd78724"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|     Name|Assignment_Submitted|\n",
            "+---------+--------------------+\n",
            "|    Alice|                true|\n",
            "|      Bob|               false|\n",
            "|Catherine|                true|\n",
            "|    David|                true|\n",
            "|      Eva|                true|\n",
            "|    Frank|               false|\n",
            "|      Ali|                true|\n",
            "|    Usman|                true|\n",
            "+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 2\n",
        "submitted_students_df = all_students_df.filter(all_students_df['Assignment_Submitted'])\n",
        "submitted_students_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCYamznFk-z-",
        "outputId": "57e3e09d-9940-4e04-9f55-60957faecad7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|     Name|Assignment_Submitted|\n",
            "+---------+--------------------+\n",
            "|    Alice|                true|\n",
            "|Catherine|                true|\n",
            "|    David|                true|\n",
            "|      Eva|                true|\n",
            "|      Ali|                true|\n",
            "|    Usman|                true|\n",
            "+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise # 3\n",
        "sorted_student_df = all_students_df.sort(all_students_df['Name'])\n",
        "sorted_student_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncF_G_IhlmVO",
        "outputId": "aab11fd0-7246-48a1-f519-a2d890e810ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|     Name|Assignment_Submitted|\n",
            "+---------+--------------------+\n",
            "|      Ali|                true|\n",
            "|    Alice|                true|\n",
            "|      Bob|               false|\n",
            "|Catherine|                true|\n",
            "|    David|                true|\n",
            "|      Eva|                true|\n",
            "|    Frank|               false|\n",
            "|    Usman|                true|\n",
            "+---------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "\n",
        "# Example 1: Creating DataFrame\n",
        "\n",
        "fruit_data = [('Apple', 1.2), ('Banana', 0.5), ('Cherry', 3.5), ('Mango', 5.0)]  # Change 5 to 5.0\n",
        "\n",
        "# Defining Schema\n",
        "fruit_schema = StructType([\n",
        "    StructField('Fruit', StringType(), True),\n",
        "    StructField('Price', FloatType(), True)\n",
        "])\n",
        "\n",
        "fruit_df = spark.createDataFrame(fruit_data, schema=fruit_schema)\n",
        "fruit_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9LnbEwumFhW",
        "outputId": "c1d08c98-2c8c-4008-a8f3-22caf0e0e024"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "| Fruit|Price|\n",
            "+------+-----+\n",
            "| Apple|  1.2|\n",
            "|Banana|  0.5|\n",
            "|Cherry|  3.5|\n",
            "| Mango|  5.0|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Adding Columns to Dataframe\n",
        "fruit_with_quality_df = fruit_df.withColumn('Quality', lit(10))\n",
        "fruit_with_quality_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVCqFQ8nnyr4",
        "outputId": "e8453f77-9ee3-4c6a-e462-e7fa9e9a4460"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+\n",
            "| Fruit|Price|Quality|\n",
            "+------+-----+-------+\n",
            "| Apple|  1.2|     10|\n",
            "|Banana|  0.5|     10|\n",
            "|Cherry|  3.5|     10|\n",
            "| Mango|  5.0|     10|\n",
            "+------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtered Dat in dataframe\n",
        "expensive_fruits_df = fruit_df.filter(fruit_df['price']>1)\n",
        "expensive_fruits_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4hJqG1BpIUO",
        "outputId": "7f940ff4-7675-40d2-f4d5-24b78a67bf3e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "| Fruit|Price|\n",
            "+------+-----+\n",
            "| Apple|  1.2|\n",
            "|Cherry|  3.5|\n",
            "| Mango|  5.0|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Removing Duplicates\n",
        "duplicate_data = [('Alice',33),('Bob',45),('Charlie',25),('Alice',33),('Charlie',25)]\n",
        "duplicate_schema=StructType([\n",
        "    StructField('Name',StringType(),True),\n",
        "    StructField('Age',IntegerType(),True)\n",
        "])\n",
        "\n",
        "duplicates_df = spark.createDataFrame(duplicate_data, schema=duplicate_schema)\n",
        "unique_df = duplicates_df.dropDuplicates()\n",
        "\n",
        "unique_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJOP0bv0s8Tb",
        "outputId": "71d234c9-db93-41a6-9ddb-c203d0898634"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 33|\n",
            "|    Bob| 45|\n",
            "|Charlie| 25|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Handling Missing Values\n",
        "# Creating a DataFrame with missing values\n",
        "missing_data = [('Alice', None), ('Bob', 45), ('Catherine', None), ('Dave',53)]\n",
        "missing_schema = StructType([\n",
        "StructField('Name', StringType(), True),\n",
        "StructField('Age', IntegerType(), True)\n",
        "])\n",
        "missing_df = spark.createDataFrame(missing_data, schema=missing_schema)\n",
        "# Filling missing values with a default value\n",
        "filled_df = missing_df.na.fill(30)\n",
        "# Displaying the DataFrame with missing values filled\n",
        "filled_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yViF9sdrveIR",
        "outputId": "d7481bbf-98a5-4d50-bffe-1fa8788b732e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 30|\n",
            "|      Bob| 45|\n",
            "|Catherine| 30|\n",
            "|     Dave| 53|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType  # Import IntegerType\n",
        "\n",
        "\n",
        "# Converting the 'Age' column to IntegerType\n",
        "converted_df = filled_df.withColumn('Age', filled_df['Age'].cast(IntegerType()))  # Pass IntegerType as a string\n",
        "# Displaying the DataFrame with converted data types\n",
        "converted_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LCBrr4bvnSP",
        "outputId": "1744d816-cd03-46c8-b65f-652bc1236800"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 30|\n",
            "|      Bob| 45|\n",
            "|Catherine| 30|\n",
            "|     Dave| 53|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Removing Duplicates\n",
        "# Creating a DataFrame with duplicate rows\n",
        "duplicate_data = [('Alice', 34), ('Bob', 45), ('Catherine', 29), ('Alice', 34)]\n",
        "duplicate_schema = StructType([\n",
        "StructField('Name', StringType(), True),\n",
        "StructField('Age', IntegerType(), True)\n",
        "])\n",
        "duplicate_df = spark.createDataFrame(duplicate_data, schema=duplicate_schema)\n",
        "# Removing duplicate rows\n",
        "unique_df = duplicate_df.dropDuplicates()\n",
        "# Displaying the DataFrame without duplicates\n",
        "unique_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aoi8mqa3ykRK",
        "outputId": "94a397cf-0016-4bea-8b59-1dee24390a1b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|      Bob| 45|\n",
            "|    Alice| 34|\n",
            "|Catherine| 29|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Filling Missing Values\n",
        "# Creating a DataFrame with missing values (None)\n",
        "missing_data = [('Alice', None), ('Bob', 45), ('Catherine', None), ('Dave',54)]\n",
        "missing_schema = StructType([\n",
        "StructField('Name', StringType(), True),\n",
        "StructField('Age', IntegerType(), True)\n",
        "])\n",
        "missing_df = spark.createDataFrame(missing_data, schema=missing_schema)\n",
        "# Filling missing values with a default age of 25\n",
        "filled_df = missing_df.fillna(25, subset=['Age'])\n",
        "# Displaying the DataFrame with missing values filled\n",
        "filled_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEWepqLUyvtV",
        "outputId": "a882c5fe-3753-4717-cd39-11bedabba6a4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 25|\n",
            "|      Bob| 45|\n",
            "|Catherine| 25|\n",
            "|     Dave| 54|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Dropping Columns\n",
        "# Creating a DataFrame with an extra column\n",
        "extra_data = [('Alice', 34, 'F'), ('Bob', 45, 'M'), ('Catherine', 29, 'F')]\n",
        "extra_schema = StructType([\n",
        "StructField('Name', StringType(), True),\n",
        "StructField('Age', IntegerType(), True),\n",
        "StructField('Gender', StringType(), True)\n",
        "])\n",
        "extra_df = spark.createDataFrame(extra_data, schema=extra_schema)\n",
        "# Dropping the 'Gender' column\n",
        "reduced_df = extra_df.drop('Gender')\n",
        "# Displaying the DataFrame without the 'Gender' column\n",
        "reduced_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtE3wRpky8eq",
        "outputId": "bb9174a9-def7-481a-95b1-54e81be25738"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 34|\n",
            "|      Bob| 45|\n",
            "|Catherine| 29|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Descriptive Statistics\n",
        "# Using the DataFrame from the previous example\n",
        "# Calculating descriptive statistics for the 'Age' column\n",
        "age_stats = reduced_df.describe('Age')\n",
        "# Displaying the descriptive statistics\n",
        "age_stats.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZIA3nBDzIqY",
        "outputId": "762e14df-ecfa-42ad-b8c0-234e9d4044b4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+\n",
            "|summary|             Age|\n",
            "+-------+----------------+\n",
            "|  count|               3|\n",
            "|   mean|            36.0|\n",
            "| stddev|8.18535277187245|\n",
            "|    min|              29|\n",
            "|    max|              45|\n",
            "+-------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Counting By Group\n",
        "# Creating a DataFrame with categories\n",
        "category_data = [('Fruit', 'Apple'), ('Fruit', 'Banana'), ('Vegetable', 'Carrot')]\n",
        "category_schema = StructType([\n",
        "StructField('Type', StringType(), True),\n",
        "StructField('Name', StringType(), True)\n",
        "])\n",
        "category_df = spark.createDataFrame(category_data, schema=category_schema)\n",
        "# Counting the number of occurrences for each 'Type'\n",
        "group_count = category_df.groupBy('Type').count()\n",
        "# Displaying the count by group\n",
        "group_count.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAbUVruyzQJ8",
        "outputId": "4e8a6be2-a213-4c59-b82f-d267cad48ffe"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|     Type|count|\n",
            "+---------+-----+\n",
            "|    Fruit|    2|\n",
            "|Vegetable|    1|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Filtering Data\n",
        "# Using the DataFrame from the previous example\n",
        "# Filtering to get only the rows where 'Type' is 'Fruit'\n",
        "fruit_df = category_df.filter(category_df.Type == 'Fruit')\n",
        "fruit_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G6JsjyrzXnn",
        "outputId": "5c875f98-0575-49d7-ed2a-970bd73fb484"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| Type|  Name|\n",
            "+-----+------+\n",
            "|Fruit| Apple|\n",
            "|Fruit|Banana|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_column_df = fruit_df.withColumn('Category', lit('Food'))\n",
        "# Displaying the DataFrame with the new column\n",
        "new_column_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ7SHsGnXb_0",
        "outputId": "57948b29-565f-4ffc-c14b-a13e3feb52de"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------+\n",
            "| Type|  Name|Category|\n",
            "+-----+------+--------+\n",
            "|Fruit| Apple|    Food|\n",
            "|Fruit|Banana|    Food|\n",
            "+-----+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example # 2\n",
        "renamed_df = new_column_df.withColumnRenamed('Name','Fruit_Name')\n",
        "renamed_df.show()"
      ],
      "metadata": {
        "id": "VAPhREpbznFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360d2d51-6617-4d6f-9b89-26b1ee6a4817"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------+\n",
            "| Type|Fruit_Name|Category|\n",
            "+-----+----------+--------+\n",
            "|Fruit|     Apple|    Food|\n",
            "|Fruit|    Banana|    Food|\n",
            "+-----+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Assuming you already have a SparkSession named 'spark' and a DataFrame named 'reduced_df'\n",
        "with_new_column_df = reduced_df.withColumn('IsAdult', col('Age') >= 18)\n",
        "with_new_column_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CntmhrVoW7y4",
        "outputId": "cf6ce8fc-3391-4b92-f62e-10513783e477"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+-------+\n",
            "|     Name|Age|IsAdult|\n",
            "+---------+---+-------+\n",
            "|    Alice| 34|   true|\n",
            "|      Bob| 45|   true|\n",
            "|Catherine| 29|   true|\n",
            "+---------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "renamed_df = reduced_df.withColumnRenamed('Age','Years')\n",
        "renamed_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOObex2gYF83",
        "outputId": "9c620a22-2758-45b6-9a07-9edef3acede1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|     Name|Years|\n",
            "+---------+-----+\n",
            "|    Alice|   34|\n",
            "|      Bob|   45|\n",
            "|Catherine|   29|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = renamed_df.filter(col('Age') >=30)\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8KDhPODYzq2",
        "outputId": "13494e4b-1434-4466-ba67-3b3dafc75cba"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "| Name|Years|\n",
            "+-----+-----+\n",
            "|Alice|   34|\n",
            "|  Bob|   45|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Assuming you already have a SparkSession named 'spark' and a DataFrame named 'reduced_df'\n",
        "average_age = reduced_df.agg(avg('Age').alias('Average_Age'))\n",
        "average_age.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AooQNZM9ZHGI",
        "outputId": "03e30caa-6f59-46a5-fd57-d4c34a3e3e87"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|Average_Age|\n",
            "+-----------+\n",
            "|       36.0|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adult_count = reduced_df.filter(col('Age') >=18).count()\n",
        "adult_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWSX617NZ1Z2",
        "outputId": "98345502-6347-42da-d456-08025e4e70eb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import max\n",
        "\n",
        "# Assuming you already have a SparkSession named 'spark' and a DataFrame named 'reduced_df'\n",
        "oldest_person = reduced_df.agg(max('Age').alias('oldest_person')).collect()\n",
        "oldest_person\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsE9U2UjakR8",
        "outputId": "0aab95d0-9fe5-4826-d59a-904650898c9d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(oldest_person=45)]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating two DataFrames\n",
        "df1 = spark.createDataFrame([(1, 'Apple'), (2, 'Banana'), (3, 'Cherry')], ['ID', 'Fruit'])\n",
        "df2 = spark.createDataFrame([(1, 'Red'), (2, 'Yellow'), (4, 'Green')], ['ID', 'Color'])\n",
        "\n",
        "# Performing an inner join on the 'ID' column\n",
        "inner_joined_df = df1.join(df2, 'ID', 'inner')\n",
        "\n",
        "# Displaying the result of the inner join\n",
        "inner_joined_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znx67mItbCws",
        "outputId": "8deae21a-22e3-4e11-9dad-16dd075e764e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| ID| Fruit| Color|\n",
            "+---+------+------+\n",
            "|  1| Apple|   Red|\n",
            "|  2|Banana|Yellow|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "left_joined = df1.join(df2,'ID','left')\n",
        "left_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nDBk8LVb9xL",
        "outputId": "01e45892-255d-4ed1-ea6e-bb39f606544f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| ID| Fruit| Color|\n",
            "+---+------+------+\n",
            "|  1| Apple|   Red|\n",
            "|  3|Cherry|  NULL|\n",
            "|  2|Banana|Yellow|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example # 1 inner join\n",
        "df1 = spark.createDataFrame([(1, 'Apple'), (2, 'Banana'), (3, 'Cherry')], ['ID', 'Fruit'])\n",
        "df2 = spark.createDataFrame([(1, 'Red'), (2, 'Yellow'), (4, 'Green')], ['ID', 'Color'])\n",
        "\n",
        "# Performing an inner join on the 'ID' column\n",
        "inner_joined_df = df1.join(df2, 'ID', 'inner')\n",
        "\n",
        "# Displaying the result of the inner join\n",
        "inner_joined_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I69XrvkNcVXE",
        "outputId": "f746891e-ec53-4aa7-8e7b-7130c839e039"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| ID| Fruit| Color|\n",
            "+---+------+------+\n",
            "|  1| Apple|   Red|\n",
            "|  2|Banana|Yellow|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example # 2 left join\n",
        "left_joined_df = df1.join(df2,'ID','left')\n",
        "left_joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG9DkK39c6tk",
        "outputId": "87bba745-a197-4180-911f-90ee9cddbfb4"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| ID| Fruit| Color|\n",
            "+---+------+------+\n",
            "|  1| Apple|   Red|\n",
            "|  3|Cherry|  NULL|\n",
            "|  2|Banana|Yellow|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example # 3 Full outer join\n",
        "full_outer_join_df = df1.join(df2,'ID','Outer')\n",
        "full_outer_join_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIW3cRMMdTvL",
        "outputId": "9ac8251a-3bc7-43ff-c38e-2f84d850afd8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| ID| Fruit| Color|\n",
            "+---+------+------+\n",
            "|  1| Apple|   Red|\n",
            "|  2|Banana|Yellow|\n",
            "|  3|Cherry|  NULL|\n",
            "|  4|  NULL| Green|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"RankingExample\").getOrCreate()\n",
        "\n",
        "# Creating a DataFrame\n",
        "salary_df = spark.createDataFrame([(1, 'Alice', 50000), (2, 'Bob', 60000), (3, 'Charlie', 55000)],\n",
        "                                  ['ID', 'Name', 'Salary'])\n",
        "\n",
        "# Defining a window specification\n",
        "windowSpec = Window.orderBy('Salary')\n",
        "\n",
        "# Applying the rank function over the window\n",
        "ranked_df = salary_df.withColumn('Rank', F.rank().over(windowSpec))\n",
        "\n",
        "# Displaying the ranked DataFrame\n",
        "ranked_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ZeZHEIdxdv",
        "outputId": "22abcb0a-0ae9-4b4d-c023-c0f11732e131"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+----+\n",
            "| ID|   Name|Salary|Rank|\n",
            "+---+-------+------+----+\n",
            "|  1|  Alice| 50000|   1|\n",
            "|  3|Charlie| 55000|   2|\n",
            "|  2|    Bob| 60000|   3|\n",
            "+---+-------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a DataFrame\n",
        "salary_df = spark.createDataFrame([(1, 'Alice', 50000),\n",
        "                                   (2, 'Bob', 60000),\n",
        "                                   (3, 'Charlie', 55000)],\n",
        "                                  ['ID', 'Name', 'Salary'])\n",
        "\n",
        "# Defining a window specification with no ordering to consider all rows\n",
        "windowSpec = Window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Calculating the max and min salary in each row's window\n",
        "max_salary = F.max('Salary').over(windowSpec)\n",
        "min_salary = F.min('Salary').over(windowSpec)\n",
        "\n",
        "# Calculating the difference between max and min salary\n",
        "salary_difference_df = salary_df.withColumn('Salary_Difference', max_salary - min_salary)\n",
        "\n",
        "# Displaying the DataFrame with salary difference\n",
        "salary_difference_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUCRYbE9eEyi",
        "outputId": "51fbbbb6-a6c0-4fe2-b9c5-7df99684e0a9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-----------------+\n",
            "| ID|   Name|Salary|Salary_Difference|\n",
            "+---+-------+------+-----------------+\n",
            "|  1|  Alice| 50000|                0|\n",
            "|  2|    Bob| 60000|            10000|\n",
            "|  3|Charlie| 55000|            10000|\n",
            "+---+-------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "null_df = spark.createDataFrame([(1, 'Apple', None), (2, None, 'Red'), (3, 'Banana', 'Yellow')],\n",
        "                                 ['ID', 'Fruit', 'Color'])\n",
        "\n",
        "# Dropping rows with any null values\n",
        "dropped_df = null_df.na.drop()\n",
        "\n",
        "# Displaying the DataFrame after dropping nulls\n",
        "dropped_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePvoGueFfHEM",
        "outputId": "41748504-4b64-4a56-e7a6-5cd7c5cb5b1b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| ID| Fruit| Color|\n",
            "+---+------+------+\n",
            "|  3|Banana|Yellow|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filed_df = null_df.na.fill({'Fruit':'Unknown','Color':'NA'})\n",
        "filled_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRNOVp2EfRMx",
        "outputId": "4e961908-a8e8-43fb-a41d-e68eb04c5a38"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 25|\n",
            "|      Bob| 45|\n",
            "|Catherine| 25|\n",
            "|     Dave| 54|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replace_df = spark.createDataFrame([(1, 'Apple', 'Unknown'), (2, 'Unknown', 'NA'), (3, 'Banana', 'Yellow')],\n",
        "                                   ['ID', 'Fruit', 'Color'])\n",
        "\n",
        "# Replacing 'Unknown' in the 'Fruit' column with 'Not Specified' and 'NA' in the 'Color' column with 'Not Available'\n",
        "replaced_df = replace_df.na.replace(['Unknown', 'NA'], ['Not Specified', 'Not Available'], subset=['Fruit', 'Color'])\n",
        "\n",
        "# Displaying the DataFrame after replacing values\n",
        "replaced_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpNaKesSf6fv",
        "outputId": "fff1001c-8ea8-4029-c5be-19691fb21cf1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+-------------+\n",
            "| ID|        Fruit|        Color|\n",
            "+---+-------------+-------------+\n",
            "|  1|        Apple|Not Specified|\n",
            "|  2|Not Specified|Not Available|\n",
            "|  3|       Banana|       Yellow|\n",
            "+---+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a DataFrame\n",
        "fruit_df = spark.createDataFrame([(1, 'Apple'), (2, 'Banana'), (3, 'Cherry')],\n",
        "                                 ['ID', 'Fruit'])\n",
        "\n",
        "# Registering the DataFrame as a SQL temporary table\n",
        "fruit_df.createOrReplaceTempView('Fruits')\n",
        "\n",
        "# Running a SQL query to select all rows from the table\n",
        "result_df = spark.sql('SELECT * FROM Fruits')\n",
        "\n",
        "# Displaying the result of the SQL query\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP3LUgN1gMRF",
        "outputId": "84ccb5c7-45e6-4408-b57d-70316f4a6ee4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| ID| Fruit|\n",
            "+---+------+\n",
            "|  1| Apple|\n",
            "|  2|Banana|\n",
            "|  3|Cherry|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = spark.sql('SELECT * FROM Fruits WHERE ID > 1')\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd_5CHmcgdDk",
        "outputId": "5db69405-443c-4d2f-8971-73e07f3b630c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| ID| Fruit|\n",
            "+---+------+\n",
            "|  2|Banana|\n",
            "|  3|Cherry|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Linear Regression with PySpark MLlib\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "# Creating a DataFrame with two columns: features and label\n",
        "data = [(1.0, 3.0), (2.0, 2.0), (3.0, 1.0)]\n",
        "df = spark.createDataFrame(data, ['feature', 'label'])\n",
        "# Converting feature column to vector type\n",
        "vec_assembler = VectorAssembler(inputCols=['feature'], outputCol='features')\n",
        "df = vec_assembler.transform(df)\n",
        "# Initializing Linear Regression model\n",
        "lr = LinearRegression(featuresCol='features', labelCol='label')\n",
        "# Fitting the model\n",
        "lr_model = lr.fit(df)\n",
        "# Displaying the coefficients and intercept\n",
        "print('Coefficients:', lr_model.coefficients)\n",
        "print('Intercept:', lr_model.intercept)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EcY0pbmgv6J",
        "outputId": "e4d0c090-80ef-4b68-c270-73ee501ae322"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-1.0000000000000027]\n",
            "Intercept: 4.000000000000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  pyspark.ml.classification import LogisticRegression\n",
        "# Creating a DataFrame with two columns: features and label\n",
        "data = [(0.0, 0.0), (1.0, 1.0), (2.0, 1.0), (3.0, 0.0)]\n",
        "df = spark.createDataFrame(data, ['feature', 'label'])\n",
        "# Converting feature column to vector type\n",
        "vec_assembler = VectorAssembler(inputCols=['feature'], outputCol='features')\n",
        "df = vec_assembler.transform(df)\n",
        "# Initializing Logistic Regression model\n",
        "logr = LogisticRegression(featuresCol='features', labelCol='label')\n",
        "# Fitting the model\n",
        "logr_model = logr.fit(df)\n",
        "# Displaying the coefficients and intercept\n",
        "print('Coefficients:', logr_model.coefficients)\n",
        "print('Intercept:', logr_model.intercept)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEG3Unazg5PP",
        "outputId": "62d56c82-5739-47ec-915d-005b07c135fd"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.0]\n",
            "Intercept: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Clustering with K-means in PySpark MLlib\n",
        "from pyspark.ml.clustering import KMeans\n",
        "# Creating a DataFrame with a single column: features\n",
        "data = [(0.0,), (1.0,), (9.0,), (10.0,)]\n",
        "df = spark.createDataFrame(data, ['feature'])\n",
        "# Converting feature column to vector type\n",
        "vec_assembler = VectorAssembler(inputCols=['feature'], outputCol='features')\n",
        "df = vec_assembler.transform(df)\n",
        "# Initializing K-means model\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "# Fitting the model\n",
        "kmeans_model = kmeans.fit(df)\n",
        "# Getting the cluster centers\n",
        "centers = kmeans_model.clusterCenters()\n",
        "# Displaying the cluster centers\n",
        "print('Cluster Centers:', centers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGxCQflnhFDb",
        "outputId": "0f68783e-7f60-44dc-82f4-e11983a43fcb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([0.5]), array([9.5])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"KMeansExample\").getOrCreate()\n",
        "\n",
        "# Creating a DataFrame with a single column: features\n",
        "data = [(0.0,), (1.0,), (9.0,), (10.0,)]\n",
        "df = spark.createDataFrame(data, ['feature'])\n",
        "\n",
        "# Converting feature column to vector type\n",
        "vec_assembler = VectorAssembler(inputCols=['feature'], outputCol='features')\n",
        "df = vec_assembler.transform(df)\n",
        "\n",
        "# Initializing K-means model with 2 clusters and seed 1\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "\n",
        "# Fitting the model\n",
        "kmeans_model = kmeans.fit(df)\n",
        "\n",
        "# Getting the cluster centers\n",
        "centers = kmeans_model.clusterCenters()\n",
        "\n",
        "# Displaying the cluster centers\n",
        "print('Cluster Centers:', centers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9bSxZ4nhWLO",
        "outputId": "06dd2fb0-2fde-4c1b-807f-a637034800b5"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([0.5]), array([9.5])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(1, 'Apple'), (2, 'Banana'), (3, 'Cherry')], ['ID', 'Fruit'])\n",
        "\n",
        "# Caching the DataFrame\n",
        "df.cache()\n",
        "\n",
        "# Performing some operations to materialize the cache\n",
        "df.count()\n",
        "\n",
        "# Repeated operation\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig83ixGBhmaW",
        "outputId": "ba0889a1-a14b-46c9-8dfc-ff5a8c92d623"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| ID| Fruit|\n",
            "+---+------+\n",
            "|  1| Apple|\n",
            "|  2|Banana|\n",
            "|  3|Cherry|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Clustering with K-means in PySpark MLlib\n",
        "from pyspark.ml.clustering import KMeans\n",
        "# Creating a DataFrame with a single column: features\n",
        "data = [(0.0,), (1.0,), (9.0,), (10.0,)]\n",
        "df = spark.createDataFrame(data, ['feature'])\n",
        "# Converting feature column to vector type\n",
        "vec_assembler = VectorAssembler(inputCols=['feature'], outputCol='features')\n",
        "df = vec_assembler.transform(df)\n",
        "# Initializing K-means model with 2 clusters\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "# Fitting the model\n",
        "kmeans_model = kmeans.fit(df)\n",
        "# Getting the cluster centers\n",
        "centers = kmeans_model.clusterCenters()\n",
        "# Displaying the cluster centers\n",
        "print('Cluster Centers:', centers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrvrjqaeh5Ga",
        "outputId": "4ef7406d-4d57-4753-bcb9-081792f68089"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([0.5]), array([9.5])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Creating two DataFrames\n",
        "df1 = spark.createDataFrame([(1, 'Apple'), (2, 'Banana'), (3, 'Cherry')], ['ID', 'Fruit'])\n",
        "df2 = spark.createDataFrame([(1, 'Red'), (2, 'Yellow'), (3, 'Red')], ['ID', 'Color'])\n",
        "\n",
        "# Performing a join using broadcast variables\n",
        "joined_df = df1.join(broadcast(df2), 'ID')\n",
        "\n",
        "# Displaying the joined DataFrame\n",
        "joined_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdpka1eAiCOv",
        "outputId": "fe2e8d40-054a-4f59-82e7-4b458ed84c69"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| ID| Fruit| Color|\n",
            "+---+------+------+\n",
            "|  1| Apple|   Red|\n",
            "|  2|Banana|Yellow|\n",
            "|  3|Cherry|   Red|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}