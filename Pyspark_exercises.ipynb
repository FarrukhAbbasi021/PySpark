{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Coding Exercises\n",
        "This notebook contains a series of challenging exercises on PySpark. Each exercise is designed to test your understanding of different PySpark concepts."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "85f35cb0-20b9-445c-9590-31a64d62bc79"
      },
      "id": "85f35cb0-20b9-445c-9590-31a64d62bc79"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYkOtHAnRqCS",
        "outputId": "1a958941-a477-4a15-bd0e-3160ef925d0e"
      },
      "id": "gYkOtHAnRqCS",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=3cc50c37b4e30026a0dbb414172146d2614b7441d19e0171acbe2ca8c015226c\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Word Count\n",
        "Given a text file, count the frequency of each word in the file. Ignore case sensitivity and punctuations."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "49bda33b-74af-4774-9c7b-5da2dc17a741"
      },
      "id": "49bda33b-74af-4774-9c7b-5da2dc17a741"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Sample text data\n",
        "sample_text = [\n",
        "    'Hello world',\n",
        "    'I am learning PySpark',\n",
        "    'This is a sample',\n",
        "    'PySpark is fun'\n",
        "]\n",
        "\n",
        "# Create an RDD from sample_text list\n",
        "text_file = sc.parallelize(sample_text)\n",
        "\n",
        "# Perform word count\n",
        "word_counts = text_file.flatMap(lambda line: line.split(' ')) \\\n",
        "                     .map(lambda word: word.lower().strip('.,!?\"')) \\\n",
        "                     .filter(lambda word: word != '') \\\n",
        "                     .map(lambda word: (word, 1)) \\\n",
        "                     .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Show the word counts\n",
        "result = word_counts.collect()\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('world', 1), ('i', 1), ('am', 1), ('learning', 1), ('this', 1), ('is', 2), ('hello', 1), ('pyspark', 2), ('a', 1), ('sample', 1), ('fun', 1)]\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fada05d-b5fd-4b29-9d2e-1b6c4acda6d9",
        "outputId": "6f405027-ca6c-4163-db62-3677a581079c"
      },
      "id": "8fada05d-b5fd-4b29-9d2e-1b6c4acda6d9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Finding Prime Numbers\n",
        "Given a list of numbers, find all the prime numbers in the list."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "a89d8076-b28d-42f8-aa4c-66cc8a2dcd08"
      },
      "id": "a89d8076-b28d-42f8-aa4c-66cc8a2dcd08"
    },
    {
      "cell_type": "code",
      "source": [
        "def is_prime(n):\n",
        "    if n <= 1:\n",
        "        return False\n",
        "    for i in range(2, int(n ** 0.5) + 1):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Sample list of numbers\n",
        "numbers = sc.parallelize(range(1, 101))\n",
        "\n",
        "# Find prime numbers\n",
        "prime_numbers = numbers.filter(is_prime)\n",
        "\n",
        "# Show the prime numbers\n",
        "prime_numbers.collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 3,\n",
              " 5,\n",
              " 7,\n",
              " 11,\n",
              " 13,\n",
              " 17,\n",
              " 19,\n",
              " 23,\n",
              " 29,\n",
              " 31,\n",
              " 37,\n",
              " 41,\n",
              " 43,\n",
              " 47,\n",
              " 53,\n",
              " 59,\n",
              " 61,\n",
              " 67,\n",
              " 71,\n",
              " 73,\n",
              " 79,\n",
              " 83,\n",
              " 89,\n",
              " 97]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f13a5450-0871-487d-bed1-58ff04fa35ea",
        "outputId": "790cb1cf-0241-48a2-a7ee-758670f656de"
      },
      "id": "f13a5450-0871-487d-bed1-58ff04fa35ea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Top N Frequent Words\n",
        "Given a text file, find the top N most frequent words in the file. Ignore case sensitivity and punctuations."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "62bd1aed-eb25-4817-8722-1a747c4aea2a"
      },
      "id": "62bd1aed-eb25-4817-8722-1a747c4aea2a"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Function to find top N frequent words\n",
        "def top_n_frequent_words(text_rdd, N):\n",
        "    word_counts = text_rdd.flatMap(lambda line: line.split(' ')) \\\n",
        "                          .map(lambda word: word.lower().strip('.,!?\"')) \\\n",
        "                          .filter(lambda word: word != '') \\\n",
        "                          .map(lambda word: (word, 1)) \\\n",
        "                          .reduceByKey(lambda a, b: a + b)\n",
        "    return word_counts.takeOrdered(N, key=lambda x: -x[1])\n",
        "\n",
        "# Sample text data\n",
        "sample_text = [\n",
        "    'Hello world',\n",
        "    'I am learning PySpark',\n",
        "    'This is a sample',\n",
        "    'PySpark is fun'\n",
        "]\n",
        "\n",
        "# Create an RDD from the sample_text list\n",
        "text_rdd = sc.parallelize(sample_text)\n",
        "\n",
        "# Find top 5 frequent words\n",
        "top_5_words = top_n_frequent_words(text_rdd, 5)\n",
        "\n",
        "# Show the top 5 frequent words\n",
        "print(top_5_words)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('is', 2), ('pyspark', 2), ('world', 1), ('i', 1), ('am', 1)]\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f40eecf-8ec0-4869-b006-f90786669c5d",
        "outputId": "b469ef86-4e6f-42b0-8b07-a8cf07687a92"
      },
      "id": "6f40eecf-8ec0-4869-b006-f90786669c5d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Inverted Index\n",
        "Given a set of text files, create an inverted index. An inverted index is a dictionary where each word is associated with a list of the document identifiers in which that word appears."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "146e9156-932e-4af8-b5e7-21c29e128928"
      },
      "id": "146e9156-932e-4af8-b5e7-21c29e128928"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Sample text data for each file\n",
        "sample_text1 = ['Hello world', 'I love PySpark']\n",
        "sample_text2 = ['PySpark is great', 'Hello everyone']\n",
        "sample_text3 = ['Great learning', 'Hello PySpark']\n",
        "\n",
        "# Create RDDs from sample text data\n",
        "text_file1 = sc.parallelize(sample_text1)\n",
        "text_file2 = sc.parallelize(sample_text2)\n",
        "text_file3 = sc.parallelize(sample_text3)\n",
        "\n",
        "# Assign IDs to text files\n",
        "text_file1 = text_file1.map(lambda line: ('file1', line))\n",
        "text_file2 = text_file2.map(lambda line: ('file2', line))\n",
        "text_file3 = text_file3.map(lambda line: ('file3', line))\n",
        "\n",
        "# Combine all text files\n",
        "all_text_files = text_file1.union(text_file2).union(text_file3)\n",
        "\n",
        "# Create inverted index\n",
        "inverted_index = all_text_files.flatMap(lambda x: [(word.lower().strip('.,!'), x[0]) for word in x[1].split()]) \\\n",
        "                                .groupByKey() \\\n",
        "                                .mapValues(list)\n",
        "\n",
        "# Show the inverted index\n",
        "result = inverted_index.collect()\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('love', ['file1']), ('learning', ['file3']), ('hello', ['file1', 'file2', 'file3']), ('pyspark', ['file1', 'file2', 'file3']), ('everyone', ['file2']), ('world', ['file1']), ('i', ['file1']), ('is', ['file2']), ('great', ['file2', 'file3'])]\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabcacef-eae4-47e5-bc1e-9da58ab6d0c9",
        "outputId": "b4cfb8db-08a9-44b1-d317-ccdba84af0b3"
      },
      "id": "fabcacef-eae4-47e5-bc1e-9da58ab6d0c9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: PageRank Algorithm\n",
        "Implement the PageRank algorithm to rank a set of webpages based on their importance."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "ee697afb-4586-4cb8-a675-6f69a4baedb2"
      },
      "id": "ee697afb-4586-4cb8-a675-6f69a4baedb2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data representing the links between webpages\n",
        "links = sc.parallelize([(1, [2, 3]), (2, [3]), (3, [1]), (4, [1, 2, 3])])\n",
        "ranks = links.map(lambda x: (x[0], 1.0))\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 10\n",
        "\n",
        "# PageRank algorithm\n",
        "for i in range(num_iterations):\n",
        "    contributions = links.join(ranks).flatMap(lambda x: [(dest, x[1][1] / len(x[1][0])) for dest in x[1][0]])\n",
        "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
        "\n",
        "# Show the PageRank of each webpage\n",
        "ranks.collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1.2446686281209396), (2, 0.6822141376476143), (3, 1.2699916385721675)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3747c26-116b-44de-aa2c-17df02926bee",
        "outputId": "d4457ba3-0b49-4f33-e2a5-2ff6a8dd5ad4"
      },
      "id": "d3747c26-116b-44de-aa2c-17df02926bee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: K-means Clustering\n",
        "Implement the K-means clustering algorithm to cluster a set of points into K clusters."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "3a017463-c993-4c1f-9683-896337f2d330"
      },
      "id": "3a017463-c993-4c1f-9683-896337f2d330"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from math import sqrt\n",
        "\n",
        "# Function to calculate Euclidean distance\n",
        "def euclidean_distance(x, y):\n",
        "    return sqrt(sum((a - b) ** 2 for a, b in zip(x, y)))\n",
        "\n",
        "# Function to assign points to the nearest centroid\n",
        "def assign_to_centroid(point, centroids):\n",
        "    distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "    return distances.index(min(distances))\n",
        "\n",
        "# Sample data points\n",
        "points = sc.parallelize([(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10)])\n",
        "\n",
        "# Initialize centroids randomly\n",
        "K = 3\n",
        "centroids = points.takeSample(False, K)\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 10\n",
        "\n",
        "# K-means algorithm\n",
        "for i in range(num_iterations):\n",
        "    cluster_assignments = points.map(lambda point: (assign_to_centroid(point, centroids), point))\n",
        "    new_centroids = cluster_assignments.groupByKey().mapValues(lambda points: tuple(sum(x) / len(points) for x in zip(*points)))\n",
        "    centroids = new_centroids.collectAsMap().values()\n",
        "\n",
        "# Show the final centroids\n",
        "centroids"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([(2.5, 2.5), (6.0, 6.0), (9.0, 9.0)])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1db0ccf-5deb-4f3c-bbd6-214568e424d0",
        "outputId": "445aac02-7651-4f95-eb6e-732ff595c834"
      },
      "id": "f1db0ccf-5deb-4f3c-bbd6-214568e424d0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 7: Collaborative Filtering\n",
        "Implement a simple collaborative filtering algorithm to make movie recommendations based on user ratings."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "06cfbe78-2940-4eea-892b-f4a7b09562d8"
      },
      "id": "06cfbe78-2940-4eea-892b-f4a7b09562d8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data representing the links between webpages\n",
        "links = sc.parallelize([(1, [2, 3]), (2, [3]), (3, [1]), (4, [1, 2, 3])])\n",
        "ranks = links.map(lambda x: (x[0], 1.0))\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 10\n",
        "\n",
        "# PageRank algorithm\n",
        "for i in range(num_iterations):\n",
        "    contributions = links.join(ranks).flatMap(lambda x: [(dest, x[1][1] / len(x[1][0])) for dest in x[1][0]])\n",
        "    ranks = contributions.reduceByKey(lambda a, b: a + b).mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
        "\n",
        "# Show the PageRank of each webpage\n",
        "ranks.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "8d92343a-3e54-454b-accb-ff75c1a41732"
      },
      "id": "8d92343a-3e54-454b-accb-ff75c1a41732"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: K-means Clustering\n",
        "Implement the K-means clustering algorithm to cluster a set of points into K clusters."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "249e48f9-a207-4636-8fbc-b4ced2b11318"
      },
      "id": "249e48f9-a207-4636-8fbc-b4ced2b11318"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Sample data representing user ratings (user_id, movie_id, rating)\n",
        "ratings = sc.parallelize([(1, 101, 5), (1, 102, 4), (1, 103, 2),\n",
        "                          (2, 101, 3), (2, 102, 4), (2, 104, 5),\n",
        "                          (3, 101, 4), (3, 103, 3), (3, 104, 4), (3, 105, 5)])\n",
        "\n",
        "# Calculate average rating for each movie\n",
        "avg_ratings = ratings.map(lambda x: (x[1], (x[2], 1))) \\\n",
        "                     .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
        "                     .mapValues(lambda x: x[0] / x[1])\n",
        "\n",
        "# Function to make recommendations\n",
        "def recommend_movies(user_id, num_recommendations):\n",
        "    user_ratings_rdd = ratings.filter(lambda x: x[0] == user_id).map(lambda x: x[1])\n",
        "    user_ratings_list = user_ratings_rdd.collect()\n",
        "    potential_movies = avg_ratings.filter(lambda x: x[0] not in user_ratings_list)\n",
        "    return potential_movies.takeOrdered(num_recommendations, key=lambda x: -x[1])\n",
        "\n",
        "# Recommend 3 movies for user 1\n",
        "recommendation_result = recommend_movies(1, 3)\n",
        "print(recommendation_result)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "67c07591-7497-49d7-a5eb-e1665db550ae"
      },
      "id": "67c07591-7497-49d7-a5eb-e1665db550ae"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from math import sqrt\n",
        "\n",
        "# Function to calculate the distance between two points\n",
        "def distance(a, b):\n",
        "    return sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n",
        "\n",
        "# Function to find the closest centroid for a given point\n",
        "def closest_centroid(point, centroids):\n",
        "    return min(range(len(centroids)), key=lambda i: distance(point, centroids[i]))\n",
        "\n",
        "# Sample data points\n",
        "points = sc.parallelize([(random.uniform(0, 10), random.uniform(0, 10)) for _ in range(100)])\n",
        "\n",
        "# Initial centroids\n",
        "initial_centroids = points.takeSample(False, 3)\n",
        "centroids = sc.broadcast(initial_centroids)\n",
        "\n",
        "# Number of iterations\n",
        "num_iterations = 10\n",
        "\n",
        "# K-means algorithm\n",
        "for i in range(num_iterations):\n",
        "    closest = points.map(lambda point: (closest_centroid(point, centroids.value), (point, 1)))\n",
        "    new_centroids = closest.reduceByKey(lambda a, b: ((a[0][0] + b[0][0], a[0][1] + b[0][1]), a[1] + b[1]))\n",
        "    new_centroids = new_centroids.mapValues(lambda x: (x[0][0] / x[1], x[0][1] / x[1])).collectAsMap()\n",
        "    centroids = sc.broadcast(new_centroids)\n",
        "\n",
        "# Show the final centroids\n",
        "centroids.value"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "8ff3161c-3449-4351-8d65-8a21f89a6199"
      },
      "id": "8ff3161c-3449-4351-8d65-8a21f89a6199"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 7: Collaborative Filtering\n",
        "Implement a simple collaborative filtering algorithm to make movie recommendations."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "544883c7-377b-4110-b502-307d78d8260d"
      },
      "id": "544883c7-377b-4110-b502-307d78d8260d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 8: Anomaly Detection\n",
        "Implement an anomaly detection algorithm to identify outliers in a dataset."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "e4b99749-9d04-49e2-87f6-11fcbe95e50e"
      },
      "id": "e4b99749-9d04-49e2-87f6-11fcbe95e50e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data points\n",
        "data_points = sc.parallelize([random.gauss(0, 1) for _ in range(100)] + [10, -10, 15, -15])\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean = data_points.mean()\n",
        "stddev = data_points.stdev()\n",
        "\n",
        "# Anomaly detection algorithm\n",
        "threshold = 3 * stddev\n",
        "anomalies = data_points.filter(lambda x: abs(x - mean) > threshold)\n",
        "\n",
        "# Show anomalies\n",
        "anomalies.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "3a6c44f7-764e-4e96-8670-63ea088adff1"
      },
      "id": "3a6c44f7-764e-4e96-8670-63ea088adff1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 9: Sentiment Analysis\n",
        "Implement a simple sentiment analysis algorithm to classify the sentiment of sentences in a text file."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "c44cca71-7100-482c-b336-1575e70d4ffd"
      },
      "id": "c44cca71-7100-482c-b336-1575e70d4ffd"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Sample sentiment words\n",
        "positive_words = sc.parallelize(['good', 'great', 'awesome', 'excellent', 'positive'])\n",
        "negative_words = sc.parallelize(['bad', 'terrible', 'awful', 'poor', 'negative'])\n",
        "\n",
        "# Sample text data\n",
        "sample_sentences = ['This is a good day',\n",
        "                    'I had a terrible experience',\n",
        "                    'What an awesome event',\n",
        "                    'This is neither good nor bad']\n",
        "\n",
        "# Create an RDD from the sample_sentences list\n",
        "text_file = sc.parallelize(sample_sentences)\n",
        "\n",
        "# Collect positive and negative words to Python lists (you only need to do this once)\n",
        "positive_list = positive_words.collect()\n",
        "negative_list = negative_words.collect()\n",
        "\n",
        "# Sentiment analysis algorithm\n",
        "sentiments = text_file.map(lambda line: line.lower()) \\\n",
        "                      .map(lambda line: (line, 1 if any(word in line for word in positive_list) else -1 if any(word in line for word in negative_list) else 0))\n",
        "\n",
        "# Show sentiments\n",
        "result = sentiments.collect()\n",
        "print(result)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "9dec82ec-e897-4774-a73b-955a99939689"
      },
      "id": "9dec82ec-e897-4774-a73b-955a99939689"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 10: Linear Regression\n",
        "Implement a simple linear regression algorithm to model the relationship between two variables."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "203762da-7d7a-4f75-840a-7b7ac883da28"
      },
      "id": "203762da-7d7a-4f75-840a-7b7ac883da28"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data points (x, y)\n",
        "data_points = sc.parallelize([(1, 2), (2, 4), (3, 3), (4, 5), (5, 4)])\n",
        "\n",
        "# Calculate the mean of x and y\n",
        "mean_x = data_points.map(lambda x: x[0]).mean()\n",
        "mean_y = data_points.map(lambda x: x[1]).mean()\n",
        "\n",
        "# Calculate the slope (b1) and intercept (b0) of the line\n",
        "b1 = data_points.map(lambda x: (x[0] - mean_x) * (x[1] - mean_y)).sum() / data_points.map(lambda x: (x[0] - mean_x) ** 2).sum()\n",
        "b0 = mean_y - b1 * mean_x\n",
        "\n",
        "# Linear regression algorithm\n",
        "predictions = data_points.map(lambda x: (x[0], b0 + b1 * x[0]))\n",
        "\n",
        "# Show predictions\n",
        "predictions.collect()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "6abc2c04-0fbd-4252-a46b-b80b1e569444"
      },
      "id": "6abc2c04-0fbd-4252-a46b-b80b1e569444"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 11: Support Vector Classification (SVC)\n",
        "Implement a simple Support Vector Classification algorithm to classify data points into two classes."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "dd03302c-a3ba-4588-8080-07c9e6150f88"
      },
      "id": "dd03302c-a3ba-4588-8080-07c9e6150f88"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Python Spark LinearSVC example\") \\\n",
        "    .config(\"spark.some.config.option\", \"config-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([0.0, 1.0]), 1.0),\n",
        "        (Vectors.dense([1.0, 0.0]), 0.0),\n",
        "        (Vectors.dense([-1.0, -1.0]), 0.0),\n",
        "        (Vectors.dense([1.0, 1.0]), 1.0)]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = spark.createDataFrame(data, [\"features\", \"label\"])\n",
        "\n",
        "# Initialize the LinearSVC model\n",
        "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
        "\n",
        "# Fit the model\n",
        "lsvcModel = lsvc.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = lsvcModel.transform(df)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "751994b4-31ee-46ed-8bba-8c4c9762721e"
      },
      "id": "751994b4-31ee-46ed-8bba-8c4c9762721e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 12: Support Vector Regression (SVR)\n",
        "Implement a simple Support Vector Regression algorithm to model the relationship between two variables."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "f956f9b0-b5b6-4111-b09e-38bc53c1af6c"
      },
      "id": "f956f9b0-b5b6-4111-b09e-38bc53c1af6c"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([0.0]), 1.0),\n",
        "        (Vectors.dense([1.0]), 2.0),\n",
        "        (Vectors.dense([2.0]), 1.0),\n",
        "        (Vectors.dense([3.0]), 3.0)]\n",
        "df = spark.createDataFrame(data, ['features', 'label'])\n",
        "\n",
        "# Initialize the LinearRegression model\n",
        "lr = LinearRegression(maxIter=10, regParam=0.1)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = lrModel.transform(df)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "314513b4-f649-4a70-b384-a32471fe2dfb"
      },
      "id": "314513b4-f649-4a70-b384-a32471fe2dfb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 13: Decision Tree Classification\n",
        "Implement a simple Decision Tree Classification algorithm to classify data points into multiple classes."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "c0999c53-fd9c-4b6d-87d1-378e2aaadc96"
      },
      "id": "c0999c53-fd9c-4b6d-87d1-378e2aaadc96"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([0.0, 1.0]), 1.0),\n",
        "        (Vectors.dense([1.0, 0.0]), 0.0),\n",
        "        (Vectors.dense([-1.0, -1.0]), 0.0),\n",
        "        (Vectors.dense([1.0, 1.0]), 1.0)]\n",
        "df = spark.createDataFrame(data, ['features', 'label'])\n",
        "\n",
        "# Initialize the DecisionTreeClassifier model\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model\n",
        "dtModel = dt.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = dtModel.transform(df)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "e79d10b3-7332-4a73-9f1f-b823be0a9b03"
      },
      "id": "e79d10b3-7332-4a73-9f1f-b823be0a9b03"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 14: Naive Bayes Classification\n",
        "Implement a simple Naive Bayes Classification algorithm to classify text documents."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "52320e6b-67c8-403f-b52e-1e5f57085b90"
      },
      "id": "52320e6b-67c8-403f-b52e-1e5f57085b90"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "\n",
        "# Sample text data\n",
        "data = [(0, 'good movie'),\n",
        "        (0, 'excellent film'),\n",
        "        (1, 'bad acting'),\n",
        "        (1, 'terrible plot')]\n",
        "df = spark.createDataFrame(data, ['label', 'text'])\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
        "wordsData = tokenizer.transform(df)\n",
        "\n",
        "# Hash the words\n",
        "hashingTF = HashingTF(inputCol='words', outputCol='features')\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "# Initialize the NaiveBayes model\n",
        "nb = NaiveBayes()\n",
        "\n",
        "# Fit the model\n",
        "nbModel = nb.fit(featurizedData)\n",
        "\n",
        "# Make predictions\n",
        "predictions = nbModel.transform(featurizedData)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "b974d7e0-398d-4edc-9189-64f5b30b3c47"
      },
      "id": "b974d7e0-398d-4edc-9189-64f5b30b3c47"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 15: Gradient Boosting\n",
        "Implement a simple Gradient Boosting algorithm to improve the performance of a weak learner."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "dbfa0774-817e-4e94-98c8-e09f773fc740"
      },
      "id": "dbfa0774-817e-4e94-98c8-e09f773fc740"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([0.0, 1.0]), 1.0),\n",
        "        (Vectors.dense([1.0, 0.0]), 0.0),\n",
        "        (Vectors.dense([-1.0, -1.0]), 0.0),\n",
        "        (Vectors.dense([1.0, 1.0]), 1.0)]\n",
        "df = spark.createDataFrame(data, ['features', 'label'])\n",
        "\n",
        "# Initialize the GBTClassifier model\n",
        "gbt = GBTClassifier(maxIter=10)\n",
        "\n",
        "# Fit the model\n",
        "gbtModel = gbt.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = gbtModel.transform(df)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "47774b8e-f1b3-420e-9edc-c1b66b54e67c"
      },
      "id": "47774b8e-f1b3-420e-9edc-c1b66b54e67c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 16: Logistic Regression\n",
        "Implement a simple Logistic Regression algorithm to classify data points into two classes."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "e33e1f81-a348-482f-acb5-4933f950ce3c"
      },
      "id": "e33e1f81-a348-482f-acb5-4933f950ce3c"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([0.0, 1.0]), 1.0),\n",
        "        (Vectors.dense([1.0, 0.0]), 0.0),\n",
        "        (Vectors.dense([-1.0, -1.0]), 0.0),\n",
        "        (Vectors.dense([1.0, 1.0]), 1.0)]\n",
        "df = spark.createDataFrame(data, ['features', 'label'])\n",
        "\n",
        "# Initialize the LogisticRegression model\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.1)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = lrModel.transform(df)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "55202ab8-1abe-4664-b095-5f1987195400"
      },
      "id": "55202ab8-1abe-4664-b095-5f1987195400"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: PySpark doesn't have a built-in Hierarchical Clustering algorithm, but you can implement it using RDDs.\n",
        "# Here's a simple example using scipy for the actual clustering part.\n",
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data points\n",
        "data = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [11, 12], [12, 13], [13, 14], [14, 15]])\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "Z = linkage(data, 'ward')\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(Z)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "e1a8ff79-cb27-49a6-8c43-e96e18117662"
      },
      "id": "e1a8ff79-cb27-49a6-8c43-e96e18117662"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 18: K-Nearest Neighbors (KNN)\n",
        "Implement a simple K-Nearest Neighbors algorithm to classify a new data point based on its nearest neighbors."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "71e6256b-544e-4b85-a0be-64fb523785cf"
      },
      "id": "71e6256b-544e-4b85-a0be-64fb523785cf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: PySpark doesn't have a built-in K-Nearest Neighbors algorithm, but you can implement it using RDDs.\n",
        "# Here's a simple example using scikit-learn for the actual KNN part.\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Sample data points and labels\n",
        "X = [[0, 1], [1, 0], [-1, -1], [1, 1]]\n",
        "y = [1, 0, 0, 1]\n",
        "\n",
        "# Initialize the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Fit the model\n",
        "knn.fit(X, y)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = knn.predict([[0.5, 0.5]])\n",
        "print('Prediction:', prediction)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "1f15791f-efa5-460b-ac8b-351747e69bba"
      },
      "id": "1f15791f-efa5-460b-ac8b-351747e69bba"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 19: Random Forest\n",
        "Implement a simple Random Forest algorithm to classify data points into multiple classes."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "cb0b161f-2003-4cd0-9cd5-a6dca124d18e"
      },
      "id": "cb0b161f-2003-4cd0-9cd5-a6dca124d18e"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([0.0, 1.0]), 1.0),\n",
        "        (Vectors.dense([1.0, 0.0]), 0.0),\n",
        "        (Vectors.dense([-1.0, -1.0]), 0.0),\n",
        "        (Vectors.dense([1.0, 1.0]), 1.0)]\n",
        "df = spark.createDataFrame(data, ['features', 'label'])\n",
        "\n",
        "# Initialize the RandomForestClassifier model\n",
        "rf = RandomForestClassifier(numTrees=10)\n",
        "\n",
        "# Fit the model\n",
        "rfModel = rf.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rfModel.transform(df)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "ca3b93d3-6592-4424-ac8c-8ee2ee221284"
      },
      "id": "ca3b93d3-6592-4424-ac8c-8ee2ee221284"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 20: AdaBoost\n",
        "Implement a simple AdaBoost algorithm to improve the performance of a weak learner."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "80f5decb-f0ff-426d-b9a7-b8e37d768776"
      },
      "id": "80f5decb-f0ff-426d-b9a7-b8e37d768776"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: PySpark doesn't have a built-in AdaBoost algorithm, but you can implement it using RDDs.\n",
        "# Here's a simple example using scikit-learn for the actual AdaBoost part.\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Sample data points and labels\n",
        "X = [[0, 1], [1, 0], [-1, -1], [1, 1]]\n",
        "y = [1, 0, 0, 1]\n",
        "\n",
        "# Initialize the weak learner\n",
        "dt = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# Initialize the AdaBoost classifier\n",
        "ab = AdaBoostClassifier(base_estimator=dt, n_estimators=10)\n",
        "\n",
        "# Fit the model\n",
        "ab.fit(X, y)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = ab.predict([[0.5, 0.5]])\n",
        "print('Prediction:', prediction)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "e2b45d5a-cf05-4968-bb3c-fa74d55868d0"
      },
      "id": "e2b45d5a-cf05-4968-bb3c-fa74d55868d0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 21: Principal Component Analysis (PCA)\n",
        "Implement Principal Component Analysis to reduce the dimensionality of a dataset."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "bc406e0d-a419-4afc-9c0d-39b6a9698d35"
      },
      "id": "bc406e0d-a419-4afc-9c0d-39b6a9698d35"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([1.0, 2.0, 3.0]),),\n",
        "        (Vectors.dense([4.0, 5.0, 6.0]),),\n",
        "        (Vectors.dense([7.0, 8.0, 9.0]),)]\n",
        "df = spark.createDataFrame(data, ['features'])\n",
        "\n",
        "# Initialize the PCA model\n",
        "pca = PCA(k=2, inputCol='features', outputCol='pca_features')\n",
        "\n",
        "# Fit the model\n",
        "pca_model = pca.fit(df)\n",
        "\n",
        "# Transform the data\n",
        "result = pca_model.transform(df)\n",
        "\n",
        "# Show the transformed data\n",
        "result.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "df5a428d-484b-4bc2-a1e4-0d19ec72e579"
      },
      "id": "df5a428d-484b-4bc2-a1e4-0d19ec72e579"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 22: Latent Dirichlet Allocation (LDA)\n",
        "Implement Latent Dirichlet Allocation to discover topics in a collection of text documents."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "b1c7024f-0f1d-43af-9f0f-068808f28da3"
      },
      "id": "b1c7024f-0f1d-43af-9f0f-068808f28da3"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([1.0, 2.0, 6.0, 0.0, 2.0]),),\n",
        "        (Vectors.dense([1.0, 3.0, 0.0, 1.0, 3.0]),),\n",
        "        (Vectors.dense([1.0, 4.0, 1.0, 0.0, 4.0]),),\n",
        "        (Vectors.dense([1.0, 2.0, 0.0, 5.0, 5.0]),),\n",
        "        (Vectors.dense([1.0, 1.0, 1.0, 4.0, 4.0]),)]\n",
        "df = spark.createDataFrame(data, ['features'])\n",
        "\n",
        "# Initialize the LDA model\n",
        "lda = LDA(k=2, maxIter=10)\n",
        "\n",
        "# Fit the model\n",
        "lda_model = lda.fit(df)\n",
        "\n",
        "# Describe topics\n",
        "topics = lda_model.describeTopics(3)\n",
        "topics.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "a1c3732d-463b-4c1a-8dbb-c6e8e8b13cc0"
      },
      "id": "a1c3732d-463b-4c1a-8dbb-c6e8e8b13cc0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 23: K-Means Clustering\n",
        "Implement K-Means Clustering to partition a dataset into clusters."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "2b06d09e-77f3-4b9e-acd2-682bb0fc7e8d"
      },
      "id": "2b06d09e-77f3-4b9e-acd2-682bb0fc7e8d"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([0.0, 0.0]),),\n",
        "        (Vectors.dense([1.0, 1.0]),),\n",
        "        (Vectors.dense([9.0, 8.0]),),\n",
        "        (Vectors.dense([8.0, 9.0]),)]\n",
        "df = spark.createDataFrame(data, ['features'])\n",
        "\n",
        "# Initialize the KMeans model\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "\n",
        "# Fit the model\n",
        "kmeans_model = kmeans.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = kmeans_model.transform(df)\n",
        "\n",
        "# Show the cluster centers\n",
        "centers = kmeans_model.clusterCenters()\n",
        "print('Cluster Centers: ', centers)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "b0041189-766f-41e7-9180-100948ed5bd9"
      },
      "id": "b0041189-766f-41e7-9180-100948ed5bd9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 24: Gaussian Mixture Model (GMM)\n",
        "Implement Gaussian Mixture Model to fit a mixture of Gaussian distributions to a dataset."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "64300b41-1a61-4b85-a479-cce260f874b0"
      },
      "id": "64300b41-1a61-4b85-a479-cce260f874b0"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import GaussianMixture\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Sample data points\n",
        "data = [(Vectors.dense([0.0, 0.0]),),\n",
        "        (Vectors.dense([1.0, 1.0]),),\n",
        "        (Vectors.dense([9.0, 8.0]),),\n",
        "        (Vectors.dense([8.0, 9.0]),)]\n",
        "df = spark.createDataFrame(data, ['features'])\n",
        "\n",
        "# Initialize the GaussianMixture model\n",
        "gmm = GaussianMixture().setK(2).setSeed(1)\n",
        "\n",
        "# Fit the model\n",
        "gmm_model = gmm.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = gmm_model.transform(df)\n",
        "\n",
        "# Show the Gaussian distributions\n",
        "gaussians = gmm_model.gaussiansDF\n",
        "gaussians.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "e3f1bcad-8ca4-4b9d-b5a1-c95ada578dad"
      },
      "id": "e3f1bcad-8ca4-4b9d-b5a1-c95ada578dad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 25: Isotonic Regression\n",
        "Implement Isotonic Regression to fit a non-decreasing function to a dataset."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "ed70add0-52c8-4801-9801-1e053c313a34"
      },
      "id": "ed70add0-52c8-4801-9801-1e053c313a34"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import IsotonicRegression\n",
        "\n",
        "# Sample data points\n",
        "data = [(1.0, 1.0), (2.0, 2.0), (3.0, 3.0), (4.0, 4.0), (5.0, 5.0), (6.0, 6.0), (7.0, 7.0)]\n",
        "df = spark.createDataFrame(data, ['feature', 'label'])\n",
        "\n",
        "# Initialize the IsotonicRegression model\n",
        "ir = IsotonicRegression(featuresCol='feature', labelCol='label')\n",
        "\n",
        "# Fit the model\n",
        "ir_model = ir.fit(df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = ir_model.transform(df)\n",
        "\n",
        "# Show the predictions\n",
        "predictions.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "62af71ef-685d-4167-83a0-f9e94172474b"
      },
      "id": "62af71ef-685d-4167-83a0-f9e94172474b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Exercises using PySpark"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "6b6c462e-2d35-4e69-935e-0b82d08db7d0"
      },
      "id": "6b6c462e-2d35-4e69-935e-0b82d08db7d0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 26: Feedforward Neural Network\n",
        "Implement a simple feedforward neural network for classification."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "3b8c2bb7-6958-4ead-a2b7-001e0b0e80b5"
      },
      "id": "3b8c2bb7-6958-4ead-a2b7-001e0b0e80b5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: PySpark doesn't have built-in support for deep learning, but you can use libraries like Elephas or TensorFlowOnSpark.\n",
        "# Here's a simple example using Keras and Elephas.\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from elephas.ml_model import ElephasEstimator\n",
        "\n",
        "# Define a Keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=784))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Initialize optimizer\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "\n",
        "# Initialize Elephas Spark ML Estimator\n",
        "estimator = ElephasEstimator()\n",
        "estimator.set_keras_model_config(model.to_yaml())\n",
        "estimator.set_optimizer_config(sgd.get_config())\n",
        "estimator.set_mode('synchronous')\n",
        "estimator.set_loss('categorical_crossentropy')\n",
        "estimator.set_metrics(['acc'])\n",
        "\n",
        "# You can now use this estimator as a Spark ML Estimator"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "cd7d8536-a08e-40d0-977c-ba07f6e3befe"
      },
      "id": "cd7d8536-a08e-40d0-977c-ba07f6e3befe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 27: Convolutional Neural Network (CNN)\n",
        "Implement a simple Convolutional Neural Network for image classification."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "d38dee44-d445-4e6d-93ae-3f8ac1265fbf"
      },
      "id": "d38dee44-d445-4e6d-93ae-3f8ac1265fbf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: PySpark doesn't have built-in support for deep learning, but you can use libraries like Elephas or TensorFlowOnSpark.\n",
        "# Here's a simple example using Keras and Elephas.\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from elephas.ml_model import ElephasEstimator\n",
        "\n",
        "# Define a Keras model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Initialize Elephas Spark ML Estimator\n",
        "estimator = ElephasEstimator()\n",
        "estimator.set_keras_model_config(model.to_yaml())\n",
        "estimator.set_optimizer_config('adam')\n",
        "estimator.set_mode('synchronous')\n",
        "estimator.set_loss('categorical_crossentropy')\n",
        "estimator.set_metrics(['acc'])\n",
        "\n",
        "# You can now use this estimator as a Spark ML Estimator"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "c7701420-4d06-4c17-aec2-f996dc0a0da9"
      },
      "id": "c7701420-4d06-4c17-aec2-f996dc0a0da9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 29: Long Short-Term Memory (LSTM)\n",
        "Implement a simple Long Short-Term Memory network for sequence prediction."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "2696421a-1f40-4319-95a1-7bcc8c152f56"
      },
      "id": "2696421a-1f40-4319-95a1-7bcc8c152f56"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: PySpark doesn't have built-in support for deep learning, but you can use libraries like Elephas or TensorFlowOnSpark.\n",
        "# Here's a simple example using Keras and Elephas.\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from elephas.ml_model import ElephasEstimator\n",
        "\n",
        "# Define a Keras model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(10, 64)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Initialize Elephas Spark ML Estimator\n",
        "estimator = ElephasEstimator()\n",
        "estimator.set_keras_model_config(model.to_yaml())\n",
        "estimator.set_optimizer_config('adam')\n",
        "estimator.set_mode('synchronous')\n",
        "estimator.set_loss('categorical_crossentropy')\n",
        "estimator.set_metrics(['acc'])\n",
        "\n",
        "# You can now use this estimator as a Spark ML Estimator"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "756b22d9-a49b-4fc8-b50d-2cf7b6718db9"
      },
      "id": "756b22d9-a49b-4fc8-b50d-2cf7b6718db9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 30: TensorFlowOnSpark: Basic Classification\n",
        "Implement a basic classification model using TensorFlowOnSpark."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "089b43f0-8f2a-47a9-98c4-774c00ef80f7"
      },
      "id": "089b43f0-8f2a-47a9-98c4-774c00ef80f7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: TensorFlowOnSpark (TFoS) allows distributed TensorFlow execution on Spark clusters.\n",
        "# Here's a simple example of a basic classification model using TensorFlowOnSpark.\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflowonspark import TFCluster\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "\n",
        "# Define TensorFlow model\n",
        "def model_fn(features, labels, mode):\n",
        "    input_layer = layers.Input(shape=(784,))\n",
        "    hidden_layer = layers.Dense(128, activation='relu')(input_layer)\n",
        "    output_layer = layers.Dense(10, activation='softmax')(hidden_layer)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    loss = tf.losses.sparse_categorical_crossentropy(labels, output_layer)\n",
        "    train_op = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
        "\n",
        "# Parse arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--batch_size', type=int, default=100)\n",
        "parser.add_argument('--epochs', type=int, default=1)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Initialize TensorFlowOnSpark\n",
        "cluster = TFCluster.run(sc, model_fn, args, num_executors=4, num_ps=1, tensorboard=True, input_mode=TFCluster.InputMode.SPARK)\n",
        "\n",
        "# Train the model\n",
        "cluster.train(train_data, args.epochs)\n",
        "\n",
        "# Shutdown the cluster\n",
        "cluster.shutdown()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "682f9ab0-2473-4821-804f-cc24e4cf2b8a"
      },
      "id": "682f9ab0-2473-4821-804f-cc24e4cf2b8a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 31: TensorFlowOnSpark: Image Classification\n",
        "Implement an image classification model using TensorFlowOnSpark."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "2c9591a5-62cc-41f6-afc2-8a0d0ac1ff30"
      },
      "id": "2c9591a5-62cc-41f6-afc2-8a0d0ac1ff30"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: TensorFlowOnSpark (TFoS) allows distributed TensorFlow execution on Spark clusters.\n",
        "# Here's a simple example of a text classification model using TensorFlowOnSpark.\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflowonspark import TFCluster\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "\n",
        "# Define TensorFlow model\n",
        "def model_fn(features, labels, mode):\n",
        "    input_layer = layers.Input(shape=(100,))\n",
        "    embedding = layers.Embedding(input_dim=5000, output_dim=128)(input_layer)\n",
        "    lstm = layers.LSTM(128)(embedding)\n",
        "    output_layer = layers.Dense(1, activation='sigmoid')(lstm)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    loss = tf.losses.binary_crossentropy(labels, output_layer)\n",
        "    train_op = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
        "\n",
        "# Parse arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--batch_size', type=int, default=100)\n",
        "parser.add_argument('--epochs', type=int, default=1)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Initialize TensorFlowOnSpark\n",
        "cluster = TFCluster.run(sc, model_fn, args, num_executors=4, num_ps=1, tensorboard=True, input_mode=TFCluster.InputMode.SPARK)\n",
        "\n",
        "# Train the model\n",
        "cluster.train(train_data, args.epochs)\n",
        "\n",
        "# Shutdown the cluster\n",
        "cluster.shutdown()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "f4bfa8f1-d24f-48db-a85e-8bb97e966261"
      },
      "id": "f4bfa8f1-d24f-48db-a85e-8bb97e966261"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 33: TensorFlowOnSpark: Autoencoder\n",
        "Implement an autoencoder model using TensorFlowOnSpark."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "97dd47a5-2370-442e-ab59-82f1e2497c1d"
      },
      "id": "97dd47a5-2370-442e-ab59-82f1e2497c1d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: TensorFlowOnSpark (TFoS) allows distributed TensorFlow execution on Spark clusters.\n",
        "# Here's a simple example of an autoencoder model using TensorFlowOnSpark.\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflowonspark import TFCluster\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "\n",
        "# Define TensorFlow model\n",
        "def model_fn(features, labels, mode):\n",
        "    input_layer = layers.Input(shape=(784,))\n",
        "    encoded = layers.Dense(128, activation='relu')(input_layer)\n",
        "    encoded = layers.Dense(64, activation='relu')(encoded)\n",
        "    decoded = layers.Dense(128, activation='relu')(encoded)\n",
        "    output_layer = layers.Dense(784, activation='sigmoid')(decoded)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    loss = tf.losses.mean_squared_error(labels, output_layer)\n",
        "    train_op = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
        "\n",
        "# Parse arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--batch_size', type=int, default=100)\n",
        "parser.add_argument('--epochs', type=int, default=1)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Initialize TensorFlowOnSpark\n",
        "cluster = TFCluster.run(sc, model_fn, args, num_executors=4, num_ps=1, tensorboard=True, input_mode=TFCluster.InputMode.SPARK)\n",
        "\n",
        "# Train the model\n",
        "cluster.train(train_data, args.epochs)\n",
        "\n",
        "# Shutdown the cluster\n",
        "cluster.shutdown()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "9841e264-fe25-46b1-ab08-ea545b3662a9"
      },
      "id": "9841e264-fe25-46b1-ab08-ea545b3662a9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 34: TensorFlowOnSpark: Generative Adversarial Network (GAN)\n",
        "Implement a Generative Adversarial Network using TensorFlowOnSpark."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "b3c3ca5d-e2d5-4c32-87a0-4aa114e76195"
      },
      "id": "b3c3ca5d-e2d5-4c32-87a0-4aa114e76195"
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: TensorFlowOnSpark (TFoS) allows distributed TensorFlow execution on Spark clusters.\n",
        "# Here's a simple example of a Generative Adversarial Network (GAN) using TensorFlowOnSpark.\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflowonspark import TFCluster\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "\n",
        "# Define TensorFlow model\n",
        "def model_fn(features, labels, mode):\n",
        "    # Generator\n",
        "    generator_input = layers.Input(shape=(100,))\n",
        "    x = layers.Dense(128, activation='relu')(generator_input)\n",
        "    x = layers.Dense(784, activation='sigmoid')(x)\n",
        "    generator = keras.Model(generator_input, x)\n",
        "\n",
        "    # Discriminator\n",
        "    discriminator_input = layers.Input(shape=(784,))\n",
        "    x = layers.Dense(128, activation='relu')(discriminator_input)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "    discriminator = keras.Model(discriminator_input, x)\n",
        "\n",
        "    # GAN\n",
        "    gan_input = layers.Input(shape=(100,))\n",
        "    gan_output = discriminator(generator(gan_input))\n",
        "    gan = keras.Model(gan_input, gan_output)\n",
        "\n",
        "    # Compile models\n",
        "    discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    discriminator.trainable = False\n",
        "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    # Training logic here...\n",
        "\n",
        "    return None  # This is a placeholder. Actual implementation will involve custom training loops.\n",
        "\n",
        "# Parse arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--batch_size', type=int, default=100)\n",
        "parser.add_argument('--epochs', type=int, default=1)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Initialize TensorFlowOnSpark\n",
        "cluster = TFCluster.run(sc, model_fn, args, num_executors=4, num_ps=1, tensorboard=True, input_mode=TFCluster.InputMode.SPARK)\n",
        "\n",
        "# Train the model\n",
        "cluster.train(train_data, args.epochs)\n",
        "\n",
        "# Shutdown the cluster\n",
        "cluster.shutdown()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "dbe30bdf-9fa6-42d9-bfd6-2286c840a194"
      },
      "id": "dbe30bdf-9fa6-42d9-bfd6-2286c840a194"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Coding Exercises using PySpark\n",
        "These exercises will cover basic programming concepts like loops, if-else statements, functions, and object-oriented programming (OOP) in PySpark."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "e7328739-1aa6-43ab-b953-0b8361f0863f"
      },
      "id": "e7328739-1aa6-43ab-b953-0b8361f0863f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 35: Using Loops in PySpark\n",
        "Write a PySpark code snippet that uses a loop to filter out even numbers from an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "497c0629-0c50-4746-a3fa-ae9adac7c05e"
      },
      "id": "497c0629-0c50-4746-a3fa-ae9adac7c05e"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Use a loop to filter out even numbers\n",
        "for i in range(2, 21, 2):\n",
        "    rdd = rdd.filter(lambda x: x != i)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "da859639-64c0-4a33-98d1-4052a5bbe7d7"
      },
      "id": "da859639-64c0-4a33-98d1-4052a5bbe7d7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 36: Using If-Else Statements in PySpark\n",
        "Write a PySpark code snippet that uses an if-else statement to filter numbers greater than 10 from an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "99fe28aa-63e2-482d-b1e3-e7c0bc83d477"
      },
      "id": "99fe28aa-63e2-482d-b1e3-e7c0bc83d477"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Use an if-else statement to filter numbers greater than 10\n",
        "filtered_rdd = rdd.filter(lambda x: x > 10 if x else None)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(filtered_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "ed99c3dc-bdb8-4208-9d80-3b62930b2dc3"
      },
      "id": "ed99c3dc-bdb8-4208-9d80-3b62930b2dc3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 37: Using Functions in PySpark\n",
        "Write a PySpark code snippet that uses a function to square the numbers in an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "661ec406-f150-4939-a277-0e8f3ce04496"
      },
      "id": "661ec406-f150-4939-a277-0e8f3ce04496"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to square a number\n",
        "def square(x):\n",
        "    return x * x\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Use the function to square the numbers in the RDD\n",
        "squared_rdd = rdd.map(square)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(squared_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "9cb386f1-0dbb-4742-8c9c-cc697b756e5f"
      },
      "id": "9cb386f1-0dbb-4742-8c9c-cc697b756e5f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 38: Using Object-Oriented Programming (OOP) in PySpark\n",
        "Write a PySpark code snippet that uses a class to encapsulate the logic for filtering even numbers from an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "827dccf3-44ce-4d3d-a3ba-ce17d1374e09"
      },
      "id": "827dccf3-44ce-4d3d-a3ba-ce17d1374e09"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class to encapsulate the logic for filtering even numbers\n",
        "class EvenNumberFilter:\n",
        "    def __init__(self, rdd):\n",
        "        self.rdd = rdd\n",
        "\n",
        "    def filter_even_numbers(self):\n",
        "        return self.rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Create an instance of the class\n",
        "even_filter = EvenNumberFilter(rdd)\n",
        "\n",
        "# Use the class method to filter even numbers\n",
        "filtered_rdd = even_filter.filter_even_numbers()\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(filtered_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "35b64a89-b621-492a-abc7-20e776d0efed"
      },
      "id": "35b64a89-b621-492a-abc7-20e776d0efed"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 39: Using Nested Loops in PySpark\n",
        "Write a PySpark code snippet that uses nested loops to create an RDD of tuples containing all pairs of numbers from two different RDDs."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "ec447b38-7c5c-4e3a-9cc4-f9c6f734f3ba"
      },
      "id": "ec447b38-7c5c-4e3a-9cc4-f9c6f734f3ba"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two RDDs\n",
        "rdd1 = sc.parallelize(range(1, 4))\n",
        "rdd2 = sc.parallelize(range(4, 7))\n",
        "\n",
        "# Use nested loops to create an RDD of tuples containing all pairs of numbers from the two RDDs\n",
        "pairs = []\n",
        "for num1 in rdd1.collect():\n",
        "    for num2 in rdd2.collect():\n",
        "        pairs.append((num1, num2))\n",
        "\n",
        "# Create an RDD from the list of pairs\n",
        "pair_rdd = sc.parallelize(pairs)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(pair_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "21b23295-a4b8-4e4a-a554-77cbce0c7b75"
      },
      "id": "21b23295-a4b8-4e4a-a554-77cbce0c7b75"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 40: Using If-Elif-Else Statements in PySpark\n",
        "Write a PySpark code snippet that uses an if-elif-else statement to categorize numbers in an RDD into 'small', 'medium', or 'large'."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "115ce63c-3b4e-4713-859d-e9f435bb234a"
      },
      "id": "115ce63c-3b4e-4713-859d-e9f435bb234a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Use an if-elif-else statement to categorize numbers\n",
        "categorized_rdd = rdd.map(lambda x: 'small' if x <= 7 else ('medium' if x <= 14 else 'large'))\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(categorized_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "b7d161cd-6d9b-4ea8-bdef-2c2cf781055a"
      },
      "id": "b7d161cd-6d9b-4ea8-bdef-2c2cf781055a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a recursive function to calculate factorial\n",
        "def factorial(n):\n",
        "    return 1 if n == 0 else n * factorial(n-1)\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 6))\n",
        "\n",
        "# Use the recursive function to calculate the factorial of numbers in the RDD\n",
        "factorial_rdd = rdd.map(factorial)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(factorial_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "85dc0695-14f0-4e58-b4eb-445c7428fe39"
      },
      "id": "85dc0695-14f0-4e58-b4eb-445c7428fe39"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 42: Using Class Inheritance in PySpark\n",
        "Write a PySpark code snippet that uses class inheritance to create a specialized filter class that filters out numbers less than 5 from an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "5272bacb-f514-472e-81d2-c5405284924c"
      },
      "id": "5272bacb-f514-472e-81d2-c5405284924c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a base class for filtering numbers\n",
        "class NumberFilter:\n",
        "    def __init__(self, rdd):\n",
        "        self.rdd = rdd\n",
        "\n",
        "    def filter_numbers(self):\n",
        "        return self.rdd\n",
        "\n",
        "# Define a specialized filter class that inherits from the base class\n",
        "class SpecializedFilter(NumberFilter):\n",
        "    def filter_numbers(self):\n",
        "        return self.rdd.filter(lambda x: x >= 5)\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Create an instance of the specialized filter class\n",
        "special_filter = SpecializedFilter(rdd)\n",
        "\n",
        "# Use the class method to filter numbers less than 5\n",
        "filtered_rdd = special_filter.filter_numbers()\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(filtered_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "ed99ece8-24c7-4b51-b27e-d223c007935c"
      },
      "id": "ed99ece8-24c7-4b51-b27e-d223c007935c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 43: Using Nested If-Else Statements in PySpark\n",
        "Write a PySpark code snippet that uses nested if-else statements to categorize numbers in an RDD into 'small', 'medium', 'large', or 'extra large'."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "9e377065-4e14-400e-bbae-763239599736"
      },
      "id": "9e377065-4e14-400e-bbae-763239599736"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Use nested if-else statements to categorize numbers\n",
        "categorized_rdd = rdd.map(lambda x: 'small' if x <= 5 else ('medium' if x <= 10 else ('large' if x <= 15 else 'extra large')))\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(categorized_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "d8150e06-6e29-4e72-92b5-4e156d56244c"
      },
      "id": "d8150e06-6e29-4e72-92b5-4e156d56244c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 44: Using For-Else Loop in PySpark\n",
        "Write a PySpark code snippet that uses a for-else loop to find the first number that is divisible by 7 in an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "ab3d7ace-c9ee-4576-b414-824e2e75f98c"
      },
      "id": "ab3d7ace-c9ee-4576-b414-824e2e75f98c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Use a for-else loop to find the first number that is divisible by 7\n",
        "for num in rdd.collect():\n",
        "    if num % 7 == 0:\n",
        "        print(f'The first number divisible by 7 is {num}')\n",
        "        break\n",
        "else:\n",
        "    print('No number is divisible by 7')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "847d3a3e-93eb-44cb-9d19-a3b0a11abc45"
      },
      "id": "847d3a3e-93eb-44cb-9d19-a3b0a11abc45"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 45: Using Function Overloading in PySpark\n",
        "Write a PySpark code snippet that uses function overloading to create two versions of a function: one that squares a number and another that cubes a number."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "7b168482-42a6-4dbd-834e-9c32d39653e3"
      },
      "id": "7b168482-42a6-4dbd-834e-9c32d39653e3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to square or cube a number\n",
        "def power(x, n=2):\n",
        "    return x ** n\n",
        "\n",
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 6))\n",
        "\n",
        "# Use the function to square the numbers in the RDD\n",
        "squared_rdd = rdd.map(lambda x: power(x))\n",
        "\n",
        "# Use the function to cube the numbers in the RDD\n",
        "cubed_rdd = rdd.map(lambda x: power(x, 3))\n",
        "\n",
        "# Collect and print the RDDs\n",
        "print('Squared RDD:', squared_rdd.collect())\n",
        "print('Cubed RDD:', cubed_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "4532f666-6039-44ec-96f2-3ef70e44823a"
      },
      "id": "4532f666-6039-44ec-96f2-3ef70e44823a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 46: Using While Loop in PySpark\n",
        "Write a PySpark code snippet that uses a while loop to find the sum of numbers in an RDD until the sum becomes greater than 50."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "14d64712-9c9e-446f-a999-69401a2d5e68"
      },
      "id": "14d64712-9c9e-446f-a999-69401a2d5e68"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 21))\n",
        "\n",
        "# Initialize sum and counter\n",
        "sum_numbers = 0\n",
        "counter = 0\n",
        "\n",
        "# Use a while loop to find the sum of numbers until the sum becomes greater than 50\n",
        "while sum_numbers <= 50:\n",
        "    sum_numbers += rdd.collect()[counter]\n",
        "    counter += 1\n",
        "\n",
        "# Print the sum and the number of elements summed\n",
        "print(f'Sum of first {counter} numbers is {sum_numbers}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "6238ce08-b7d2-451d-879d-1231b3ba8960"
      },
      "id": "6238ce08-b7d2-451d-879d-1231b3ba8960"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 47: Using Function Composition in PySpark\n",
        "Write a PySpark code snippet that uses function composition to apply multiple transformations to an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "d3cdd557-8782-46bf-8afb-d67ef39cba01"
      },
      "id": "d3cdd557-8782-46bf-8afb-d67ef39cba01"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Define functions for transformations\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "def add_one(x):\n",
        "    return x + 1\n",
        "\n",
        "# Use function composition to apply multiple transformations\n",
        "transformed_rdd = rdd.map(square).map(add_one)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(transformed_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "cebd1e90-1f85-42e8-b995-0b94008bb1cd"
      },
      "id": "cebd1e90-1f85-42e8-b995-0b94008bb1cd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 48: Using Function as a Parameter in PySpark\n",
        "Write a PySpark code snippet that uses a function as a parameter to another function to apply transformations to an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "49befabb-2334-4cf0-8202-fb75df9ba81e"
      },
      "id": "49befabb-2334-4cf0-8202-fb75df9ba81e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Define a function that takes another function as a parameter\n",
        "def apply_transformation(rdd, func):\n",
        "    return rdd.map(func)\n",
        "\n",
        "# Define a function to square a number\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "# Use the function that takes another function as a parameter\n",
        "transformed_rdd = apply_transformation(rdd, square)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(transformed_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "9ac8c7b7-8267-4b2e-8025-f1190c90654a"
      },
      "id": "9ac8c7b7-8267-4b2e-8025-f1190c90654a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 49: Using Nested Functions in PySpark\n",
        "Write a PySpark code snippet that uses nested functions to apply a sequence of transformations to an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "d650fb3e-67aa-4024-98c1-4034f6bf4947"
      },
      "id": "d650fb3e-67aa-4024-98c1-4034f6bf4947"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Define a function with nested functions\n",
        "def apply_nested_transformations(rdd):\n",
        "    def square(x):\n",
        "        return x ** 2\n",
        "    def add_one(x):\n",
        "        return x + 1\n",
        "    return rdd.map(square).map(add_one)\n",
        "\n",
        "# Use the function with nested functions to apply transformations\n",
        "transformed_rdd = apply_nested_transformations(rdd)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(transformed_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "72e48786-58f6-4833-ba3e-02638674c63d"
      },
      "id": "72e48786-58f6-4833-ba3e-02638674c63d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 50: Using Anonymous Functions in PySpark\n",
        "Write a PySpark code snippet that uses anonymous (lambda) functions to apply transformations to an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "1a422d5c-63f6-4ce9-a14c-8b0ddd2f9e16"
      },
      "id": "1a422d5c-63f6-4ce9-a14c-8b0ddd2f9e16"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Use anonymous functions to apply transformations\n",
        "transformed_rdd = rdd.map(lambda x: x ** 2).map(lambda x: x + 1)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(transformed_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "887dc416-ba29-4413-bb55-4d7d95997c15"
      },
      "id": "887dc416-ba29-4413-bb55-4d7d95997c15"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Define a function that takes another function as a parameter\n",
        "def apply_transformation(rdd, func):\n",
        "    return rdd.map(func)\n",
        "\n",
        "# Define a function for transformation\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "# Use the function as a parameter to apply transformation\n",
        "transformed_rdd = apply_transformation(rdd, square)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(transformed_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "4b15c9eb-1198-4d2e-b7a7-df95a9f146a3"
      },
      "id": "4b15c9eb-1198-4d2e-b7a7-df95a9f146a3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 49: Using Function Chaining in PySpark\n",
        "Write a PySpark code snippet that uses function chaining to apply multiple transformations to an RDD in a single line."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "3769f078-38fd-4a19-bb8e-f523504905f3"
      },
      "id": "3769f078-38fd-4a19-bb8e-f523504905f3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Define functions for transformations\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "def add_one(x):\n",
        "    return x + 1\n",
        "\n",
        "# Use function chaining to apply multiple transformations in a single line\n",
        "transformed_rdd = rdd.map(square).map(add_one)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(transformed_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "c3d9bd16-78b0-4bfa-912d-cacb389765a8"
      },
      "id": "c3d9bd16-78b0-4bfa-912d-cacb389765a8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 50: Using Anonymous Functions in PySpark\n",
        "Write a PySpark code snippet that uses an anonymous function (lambda function) to apply a transformation to an RDD."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "6dd05893-b6cb-4ae3-8dbe-3a71abb6ff5d"
      },
      "id": "6dd05893-b6cb-4ae3-8dbe-3a71abb6ff5d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD\n",
        "rdd = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Use an anonymous function to square the numbers in the RDD\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)\n",
        "\n",
        "# Collect and print the RDD\n",
        "print(squared_rdd.collect())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "189fe8d2-c82c-42e8-95d4-13cb7c5f61f9"
      },
      "id": "189fe8d2-c82c-42e8-95d4-13cb7c5f61f9"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_delta_id": "34ddbcf7-6033-4373-95bc-068593087b66"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9",
      "language": "python",
      "language_version": "3.9",
      "identifier": "legacy"
    },
    "selected_hardware_size": "small",
    "nteract": {
      "version": "noteable@2.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}